{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6d1bcWs-tgvb"
   },
   "source": [
    "## **Notebook for Student**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESOYxiYmt7aC"
   },
   "source": [
    "##**Tasks 1 Data Acquisition and Data Source **\n",
    "\n",
    "This task on data acquisition and data source is mainly to focus on how to properly acquire and read the data with differnt formats.\n",
    "\n",
    "To do:\n",
    "\n",
    "*   Read the given data for required format; \n",
    "*   Using Numpy and Pandas to do the ETL for the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRjF0oFYztMp"
   },
   "source": [
    "**Task 1.1** We first read the given data source. However, it is not in a standard readable format. The parquet file is in columnar storage format for efficient storage purpose (very common in cloud service scenario). Our goal in this task is to read this parquet file.\n",
    "\n",
    "**Background:**\n",
    "Parquet is an open source file format available to any project in the Hadoop ecosystem. Apache Parquet is designed for efficient as well as performant flat columnar storage format of data compared to row based files like CSV or TSV files. -- [check link](https://databricks.com/glossary/what-is-parquet)\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "There are many ways to read the parquet file:\n",
    "\n",
    "\n",
    "1.   Using pyarrow engine to read   \n",
    "        ```\n",
    "        ***pyarrow.parquet.ParquetFile(filename)***\n",
    "        ```\n",
    "2.   Directly read parquet with pandas  \n",
    "        ```\n",
    "       ***pandas.read_parquet(filename, engine='pyarrow')***\n",
    "        ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k6EUXOxC8TYg"
   },
   "outputs": [],
   "source": [
    "# read the parquet file by using pyarrow \n",
    "import pyarrow.parquet as pq\n",
    "parquet_file = pq.ParquetFile('data/bank.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M9EbsD_J9dMY",
    "outputId": "c761defb-74d1-4636-9102-759d7ff00ec0"
   },
   "outputs": [],
   "source": [
    "# print the parquet file schema to check the columns and its datatype\n",
    "#parquet_file.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JTQNbJ8v_oCQ",
    "outputId": "d2c73d1f-d3c1-4caa-e2b6-37343b08c72c"
   },
   "outputs": [],
   "source": [
    "# print the metadata for the parquet file\n",
    "#parquet_file.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWzs-ofl-b1A"
   },
   "outputs": [],
   "source": [
    "# convert the parquet file into table format and print it in dataframe type\n",
    "table = parquet_file.read()\n",
    "df = table.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_FMMSTN-eOo"
   },
   "outputs": [],
   "source": [
    "#print the table into dictionary format. comment the %%capture to see the print output\n",
    "%%capture\n",
    "dic = table.to_pydict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "rcPivgPYrdcV",
    "outputId": "ee90327e-89af-41ba-8f23-a7db9109f308"
   },
   "outputs": [],
   "source": [
    "# Directly read the parquet file by uing pandas\n",
    "import pandas as pd\n",
    "df_ = pd.read_parquet('data/bank.parquet', engine='pyarrow')\n",
    "df_.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlneoNaiCU_e"
   },
   "source": [
    "**Task 1.2** After having the dataframe for the given data, let's first focus on the dictionary format -- which is the common used format while for semi-structure data in frontend development. **A record in dictionary  (one row in csv format) is now partitioned on column level**. \n",
    "We will try to format the dictionary by only selecting the **records (rows)** with age > 50 and age < 70  and then store it as a json file.\n",
    "\n",
    "**Background:** \n",
    "\n",
    "Dictionaries are used to store data values in key:value pairs.\n",
    "A dictionary is a collection which is ordered*, changeable and does not allow duplicates. -- [check link](https://www.w3schools.com/python/python_dictionaries.asp)\n",
    "\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "```\n",
    "for key, val in d.items():\n",
    "    if filter_string not in key:\n",
    "        continue\n",
    "    do something\n",
    "\n",
    "```\n",
    "\n",
    "or \n",
    "```\n",
    "filtered_dict = {k:v for (k,v) in d.items() if filter_string in k}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2bzhRy5sGw5w",
    "outputId": "7e82136c-b3e3-4f92-bba8-f6af59d0b506"
   },
   "outputs": [],
   "source": [
    "# Let's first print out all the keys and the length of the values in the given data dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UiTELvRQM1Vl"
   },
   "outputs": [],
   "source": [
    "# let's print the keys and also the unique value from the values in in each (k,v) pair in the given data dictionary\n",
    "# unique value of the array could be calculated via numpy.unique(array,return_counts = False)\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7MEor6d_HzY1",
    "outputId": "72810ac6-aa72-4145-e196-b82be9531639"
   },
   "outputs": [],
   "source": [
    "# Let's then print both keys and the length of the unique value from the values in each (k,v) pair in the given data dictionary\n",
    "# for example, in the key value pair {fruit: ['apple','pear','banana']}, the length of the unique value is 3\n",
    "# unique value of the array could be calculated via numpy.unique(array,return_counts = False)\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IY4mwSapI579"
   },
   "outputs": [],
   "source": [
    "# Now let's filter the dictionary, by doing it, you need to first create a null dictionary \n",
    "# and then write the filtered key value pair in the null dictionary\n",
    "# The code is given as below\n",
    "\n",
    "newDict = dict()\n",
    "for k,v in dic.items():\n",
    "   # Check if value meets the condition on particular key\n",
    "    if k == 'age':\n",
    "        flt = filter(lambda age: age < 70 and age >50, np.array(v).astype(int))\n",
    "        new_v = list(flt)\n",
    "        newDict[k] = new_v\n",
    "    else:\n",
    "        newDict[k] = v\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-UwGAzDhL4td",
    "outputId": "d6c56797-bf4a-4dfd-f1ce-0e9757a13bfe"
   },
   "outputs": [],
   "source": [
    "# let's double check the results\n",
    "for k, v in newDict.items():\n",
    "  print(k, np.unique(v,return_counts=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRhcaCCzNIDH"
   },
   "source": [
    "**Question here for 1.2** Have we finished the task 1.2? if yes, why? if no, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtcgScf2Npld"
   },
   "outputs": [],
   "source": [
    "# Now let's redo the filtering on the dictionary, by doing it, you need to first create a null dictionary \n",
    "# and then write the filtered key value pair in the null dictionary\n",
    "# Please write code as below again:\n",
    "\n",
    "newDict = dict()\n",
    "index_col = [i for i in range(len(dic['age'])) if (dic['age'][i] > 50) and (dic['age'][i] <70)]\n",
    "for k,v in dic.items():   \n",
    "    newDict[k] = list(np.array(v)[index_col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3kYq02MHPCEt",
    "outputId": "6607b76e-312f-4aef-b814-10af7a43fbc4"
   },
   "outputs": [],
   "source": [
    "#let's check the length of the values again, do we see the difference?\n",
    "for k, v in newDict.items():\n",
    "  print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q83knHKOSZuz"
   },
   "outputs": [],
   "source": [
    "# let's store the filtered result into json,\n",
    "# the numpy to json encoder is provided\n",
    "import json\n",
    "def np_encoder(object):\n",
    "    if isinstance(object, np.generic):\n",
    "        return object.item()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "id": "BFFiZNUlTd_U",
    "outputId": "fca78834-2c22-437c-a2f0-0f38afc5fce8"
   },
   "outputs": [],
   "source": [
    "#let's read it via pandas to check\n",
    "df_json = pd.read_json('data/bank.json')\n",
    "df_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdWCwUPHTvD7"
   },
   "source": [
    "**Task 1.3** After reading the data and also filter the dictionary, we finally have our json file and store it in our repo. However, if we need to regularly perform the similarly tasks for acquire the datasource, a parser is required. \n",
    "Could you write a parser function to contain the above parquet reading and dictionary filtering functionalities?\n",
    "\n",
    "**Background:** \n",
    "\n",
    "Python parser is mainly used for converting data in the required format, this conversion process is known as parsing. As in many different applications data obtained can have different data formats and these formats might not be suitable to the particular application and here comes the use of parser that means parsing is necessary for such situations. Therefore, parsing is generally defined as the conversion of data with one format to some other format is known as parsing -- [check link](https://www.educba.com/python-parser/)\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "```\n",
    "def parser(datasource_path,dataoutput_path,filter_key, min, max):\n",
    "    parquet_file = pq.ParquetFile(datasource_path)\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IArKkmBTuGD"
   },
   "outputs": [],
   "source": [
    "#Code for the parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OrWR-B-rWWUT"
   },
   "outputs": [],
   "source": [
    "# test the parser with condition on min=30, max=35 for age. Name the new json file as 'newbank.json'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsvEDRCoXqs1"
   },
   "source": [
    "**Task 1.4** Now we have the parser and also have the json file, then next step for us is doing the ETL by using numpy. \n",
    "Let's first understand the most important numpy operation -- Broadcasting.\n",
    "In this task, please use the numpy to add 10 to every age in 'newbank.json'.\n",
    "\n",
    "**Background:**\n",
    "\n",
    "Broadcasting is a process performed by NumPy that allows mathematical operations to work with objects that don't necessarily have compatible dimensions -- [check link](https://numpy.org/doc/stable/user/basics.broadcasting.html)\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "```\n",
    "array_a = np.array([[1, 2],\n",
    "                    [3, 4]])\n",
    "array_a + 1\n",
    "```\n",
    "Output:\n",
    "```\n",
    "array([[2, 3],\n",
    "       [4, 5]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "id": "iqppvTeocK7q",
    "outputId": "4b283ba6-08f7-4a20-8946-c9c17ea0f27c"
   },
   "outputs": [],
   "source": [
    "# firstly, let's read the newbank.json into dataframe by using pandas\n",
    "import pandas as pd\n",
    "df_newbank = pd.read_json('data/newbank.json')\n",
    "df_newbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Amd84PRRdzQS",
    "outputId": "d013428c-3d39-465e-ff00-a10ab49c7f45"
   },
   "outputs": [],
   "source": [
    "# let's convert the dataframe into numpy -- np_df and print the shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sA17jlcweB_S",
    "outputId": "0d8d3f6f-cdd9-473c-8913-219f88326bba"
   },
   "outputs": [],
   "source": [
    "# let's slicing the age from the np_df as np_df_age and print the shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93jVauJTeWmZ",
    "outputId": "80416b88-b9ba-48f5-cbff-3e511d5c82d5"
   },
   "outputs": [],
   "source": [
    "# let's use the np.broadcast to add 10 on each age and save the result as np_df_age_new, also print the shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zz3REUXier2S",
    "outputId": "07d7c35b-1a5b-43c4-b551-c5fe699e69d1"
   },
   "outputs": [],
   "source": [
    "#let's add the np_df_age_new back to np_df and save it as np_df_new, also print the shape\n",
    "# add the np_df_age_new to np_df will need to perform on column level which is the axis 1. Also the shape of the two array needs to be same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "7d8AyP7ggLob",
    "outputId": "a8a0a854-5401-48c6-cfc9-3bcb064c190c"
   },
   "outputs": [],
   "source": [
    "# let's convert the numpy array to dataframe -- df_newbank_new and print the first 5 rows\n",
    "# also give the new column a name as 'new_age'\n",
    "# make sure new column is on the first column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZoNmP_ihVws"
   },
   "source": [
    "**Task 1.5** Could we devide the numerical columns and categorical columns from df_?\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "select_dtypes('number') could be used here to select the numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpVfYCMJiCMx"
   },
   "outputs": [],
   "source": [
    "# firstly, let's select the numerical columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ObpCyGcui8y7"
   },
   "outputs": [],
   "source": [
    "# then let's remove the numerical columns from all columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "QL34qSctjQla",
    "outputId": "5d7c7f0a-0681-47c4-ad48-143281bf1841"
   },
   "outputs": [],
   "source": [
    "# let's slice the data by only selecting the numerical columns, save the new dataframe as df_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "iyn2nsjSkFNh",
    "outputId": "4491e673-df2e-480d-a9b0-9befd2ede832"
   },
   "outputs": [],
   "source": [
    "# then let's slice the data by only selecting the category columns, save the new dataframe as df_cat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSADa42UkV3g"
   },
   "source": [
    "**Task 1.6 (Advanced)** Now we have divided the df_ into two part, numerical dataframe and categorical dataframe. For many big analysis, category data is not the best format to start with. The common way to deal category data is to transform it to one hot encode format (only with 1 and 0). Could you finish the one hot encode transforming for categorical dataframe by using the provided code in hint?\n",
    "\n",
    "**Background:**\n",
    "\n",
    "One-hot Encoding is a type of vector representation in which all of the elements in a vector are 0, except for one, which has 1 as its value, where 1 represents a boolean specifying a category of the element. -- [check link](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "```\n",
    "df_onehot = pd.get_dummies(s)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "5eJL23JdkVDj",
    "outputId": "68b44a19-6ca4-4589-96e7-bc8f8dd5b567"
   },
   "outputs": [],
   "source": [
    "# let's use the code in hint to do the one hot encode and print the dataframe\n",
    "#df_onehot = pd.get_dummies(df_cat)\n",
    "#df_onehot.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "9XnnuI2jnNSw",
    "outputId": "f6ef0c0b-ead8-473e-bc6a-7ced599afed6"
   },
   "outputs": [],
   "source": [
    "# let's combine the new onehot encode dataframe with numerical dataframe to obtain the full dataframe df_all\n",
    "\n",
    "#df_all = pd.concat([df_num,df_onehot],axis=1)\n",
    "#df_all.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b18WQBJWngUl"
   },
   "source": [
    "##**Tasks 2 Advanced Data Acquisition **\n",
    "\n",
    "This task on Advanced data acquisition is to use numpy and pandas to perform more advanced Code-based ETL.\n",
    "\n",
    "To do:\n",
    "\n",
    "*   Create a function to calculate the euclidean distance between recoard; \n",
    "*   Find the most similar record for each one in the bank data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKV9ocD0odVQ"
   },
   "source": [
    "**Task 2.1** In numpy, the euclidean distance could be calculated via \n",
    "```\n",
    "np.sqrt(np.sum(np.square(point1 - point2)))\n",
    "```\n",
    "point1 and point2 is the 1D array, please folllow the above calculation and build a function to calculate the euclidean distance for any two arrays from a given dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8CqplgpoXuQ"
   },
   "outputs": [],
   "source": [
    "# define the funtion as below with name \"dist_func\"\n",
    "\n",
    "def dist_func(row1,row2):\n",
    "  return np.sqrt(np.sum(np.square(row1 - row2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BFB7noOktiV6"
   },
   "outputs": [],
   "source": [
    "# what about if point2 is a 2d array? how to calculate the distance from point1 to each dimension of point2?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeLpRt27qyaP"
   },
   "source": [
    "**Task 2.2** Now, we will need to calculate the euclidean distance between each row and all the rows (let's include the current row at here), also we would like to save the distances into array for each row. To the end, you will have a distance matrix with shape of (n,n) where n is the total rows.\n",
    "We will use top 100 rows from *df_all* as the input.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "Use the for loop on each row could be a good start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_yD02FtqYbs"
   },
   "outputs": [],
   "source": [
    "# let's write the code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eqXg75YIv2JP",
    "outputId": "664caed0-f72a-42ee-e4be-fb5c0af82535"
   },
   "outputs": [],
   "source": [
    "# let's print the distance matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6NrYStXwIg_"
   },
   "outputs": [],
   "source": [
    "# let's find the index of the smallest distance for each row in the distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "id": "bQnvYx2_x3lu",
    "outputId": "4c398176-8ce1-493d-9480-6e303671c76c"
   },
   "outputs": [],
   "source": [
    "#let's put the results into pandas dataframe\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "M04Prac_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
