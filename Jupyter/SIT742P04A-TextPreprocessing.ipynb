{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ht8cT-Ou6a7R"
   },
   "source": [
    "# SIT742: Modern Data Science \n",
    "**(Week 04: Text Analysis)**\n",
    "\n",
    "---\n",
    "- Materials in this module include resources collected from various open-source online repositories.\n",
    "- You are free to use, change and distribute this package.\n",
    "- If you found any issue/bug for this document, please submit an issue at [tulip-lab/sit742](https://github.com/tulip-lab/sit742/issues)\n",
    "\n",
    "Prepared by **SIT742 Teaching Team**\n",
    "\n",
    "---\n",
    "\n",
    "## Session 4A - The Fundamentals of Text Pre-processing\n",
    "\n",
    "Table of Content\n",
    "\n",
    "* Part 1. Accessing Various Text Resources\n",
    "* Part 2. Basic Steps of Pre-Processing Text \n",
    "* Part 3. Summary\n",
    "* Part 4. Reading Materials\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "The majority of text data that appears in everyday sources such as books, \n",
    "newspapers, magazines, emails, blogs, and tweets \n",
    "is free language text. Given the amount of information stored as text on the Internet, it is not feasible for a human to manually explore such a large amount of text data to extract useful information. Therefore, we have to use automatic approaches, such as text analysis algorithms developed in the fields of text mining, natural language process (NLP) and information retrieval (IR). It is worth knowing that computers cannot directly understand text like humans. For example, humans can automatically break down sentences into units of meaning, but computers cannot. Therefore, text data must be processed before various text analysis algorithms can use it.\n",
    "\n",
    "Unlike the data you can retrieve from relational databases, text data always appears in an unstructured form.\n",
    "By unstructured we mean that text data exists \"in the wild\" and has not been converted into a structured format, like a spreadsheet. Therefore, it has to be manipulated and converted into a proper structured and numerical format consumable by text analysis algorithms, which is referred to as text pre-processing. It is an important task and a critical step in text analysis. The characters, words and sentences identified by text pre-processing are the fundamental units passed to all the downstream text analysis algorithms, such as part-of-speech tagging, parsing, document classification and clustering, etc.\n",
    "This chapter describes the basic pre-processing steps that are needed to convert unstructured text into a structured \n",
    "format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "et29QNJy6a7X"
   },
   "source": [
    "## Part 1. Accessing Various Text Resources\n",
    "\n",
    "What are the text corpora and lexical resources often used in text analysis? Where and how can we \n",
    "access them? \n",
    "Text data used for different text analysis tasks can be derived from various resources, such as \n",
    "* **Existing data repositories**, most of which contains corpora that have been either pre-processed into a specific format that can be directly digested by the downstream text analysis algorithms or manually annotated. \n",
    "For example,\n",
    "    *  [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.html?format=&task=&att=&area=&numAtt=&numIns=&type=text&sort=nameUp&view=table) contains 30 corpora that can be used in text mining tasks, such as regression, clustering, and classification.  \n",
    "    * [Linguistic Data Consortium](https://www.ldc.upenn.edu/) contains corpora mainly used in various natural language processing tasks, such as parsing, acoustic analysis, phonological analysis and etc. One disadvantage of using LDC is that its corpora are not free. Users have to buy a license in order to use those corpora.\n",
    "    \n",
    "* **NLTK**: A language toolkit that also includes a diverse set of corpora and lexical resources, which include, for example,\n",
    "    * Plain text corpora, e.g.,\n",
    "        * The Gutenberg Corpus contains thousands of books.\n",
    "    * Tagged Corpora, e.g.,\n",
    "        * The Brown Corpus is annotated with part-of-speech tags. Each word is now paired with its part-of-speech tag.\n",
    "           You can retrieve words as (word, tag) tuples, rather than just bare word strings.\n",
    "    * Chunked Corpora, e.g.,\n",
    "        * The CoNLL corpora includes phrasal chunks (CoNLL 2000), named entity chunks (CoNLL 2002).\n",
    "    * Parsed Corpora, e.g.,\n",
    "        * The Treebank corpora provide a syntactic parse for each sentence, like the Penn Treebank based on Wall Street Journal samples.\n",
    "    * Word List and Lexicons, e.g.,\n",
    "        * [WordNet](https://wordnet.princeton.edu/): a large lexical database of English, where nouns, verbs, adjectives and adverbs are organized into interlinked synsets (i.e., sets of synonyms)\n",
    "    * Categorized Corpora: \n",
    "        * The Reuters corpus: a corpus of Reuters News stories for used in developing text analysis algorithms.\n",
    "        \n",
    "* **Web**: The largest source for getting text data is the Web. Text can be extracted from webpages or be retrieved\n",
    "via various APIs. For example,\n",
    "     * **Wikipedia articles**: The Wikimedia website provides links to download dumps of Wikipedia articles. Click [here](https://dumps.wikimedia.org/enwiki) to view various dumps for English Wikipedia articles. \n",
    "     * **Tweets** that allows people to communicate with short, 140-characters messages. It is fortunate that Twitter provides quite well documented API that we can use to retrieve tweets of our interest.\n",
    "     * The other text data can be scraped from the Internet, like webpages. Here is a <a href=\"https://www.youtube.com/watch?v=3xQTJi2tqgk\">Youtube video</a> on **scraping websites with Python**.\n",
    "\n",
    "The set of NLTK corpora can be easily accessed with interfaces offered by NLTK. Here we show you how to install the text data that comes with NLTK and all the packages included in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FyjDKH_y6a7b"
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "#If you're unsure of which data/model you need, you can start out with the basic list of data + models with:\n",
    "#It will download a list of \"popular\" resources, these includes:\n",
    "nltk.download(\"popular\")\n",
    "#It will download a list of \"retuters\" resources, thses includes:\n",
    "nltk.download(\"reuters\")\n",
    "#While you downliad the nltk package, it will show the Download path,(root/nltk_data)\n",
    "#It will also show the 1st item in the nltk.data.path list\n",
    "\n",
    "# Specifies the file stored in the NLTK data package at *path*. NLTK will search for these files in the directories specified by ``nltk.data.path``.\n",
    "nltk.data.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1729
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15346,
     "status": "error",
     "timestamp": 1552971128623,
     "user": {
      "displayName": "LIU ZHI",
      "photoUrl": "",
      "userId": "02817931634636469244"
     },
     "user_tz": -660
    },
    "id": "WyTNm7Ge6a7m",
    "outputId": "e11fda80-5c7b-458b-8ea4-8864c3a1aca4"
   },
   "outputs": [],
   "source": [
    "# import nltk \n",
    "#A new window should open, showing the NLTK Downloader.\n",
    "#You can input the related character for the command.\n",
    "#For example, if you would like to check the current NLTK configuation details.\n",
    "#Just follow the prompt message, such as input 'c' to check configuration\n",
    "# and then input `m', and 'q' to exit.\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zC1Msn826a7w"
   },
   "source": [
    "You also can install the NLTK software on the Mac or Windows OS, For example, if you use the Mac OS, then you run the above block's two commands and it will locally gives you a window that looks like the following screenshot. For this lab, we use the Google Colab, a Linux-like system, it will show the command line interface (CLL) not a windows as below if you run the nltk.download() function.\n",
    "\n",
    "![NLTK](https://github.com/tulip-lab/sit742/raw/master/Jupyter/image/nltkInstallWindow.png \"NLTK\")\n",
    "\n",
    "\n",
    "\n",
    "This window,  'NTLK Download', shown on the Mac OS will allows you to browse the available corpora and packages included in NLTK. The Collections tab on the downloader shows how the packages are grouped into sets. You can select the line labeled \"all\" and click \"download\" to obtain all corpora and packages (<font color = \"red\">Warning: the size is a couple of GBs</font>). It will take a couple of minutes to download the corpora and packages, depending on how fast your Internet connection is. You can also choose to just install the copora and packages as you go.\n",
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v98Jv-IM6a7z"
   },
   "source": [
    "## Part 2. Basic Steps of Pre-Processing Text\n",
    "\n",
    "The possible steps of text pre-processing are nearly the same for all text analysis tasks, though which pre-processing steps are chosen depends on the specific task. The basic steps are as follows:\n",
    "* Tokenization\n",
    "* Case normalization\n",
    "* Removing Stop words\n",
    "* Stemming and Lemmatization\n",
    "* Sentence Segmentation\n",
    "\n",
    "We will walk you through each of these steps with some examples. First, you need to \n",
    "decide <font color=\"red\">the scope of the text to be used in the downstream text analysis tasks</font>. Should you use an entire document?\n",
    "Or should you break the document down into sections, paragraphs, or sentences. Choosing \n",
    "the proper scope depends on the goals of the analysis task.\n",
    "For example, you might choose to use an entire document in document classification and clustering tasks\n",
    "while you might choose smaller units like paragraphs or sentences in document summarization and information\n",
    "retrieval tasks. The scope chosen by you will have an impact on the steps needed in the pre-processing process.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jR2B1MS_K86m"
   },
   "source": [
    "### 2.1. Tokenization\n",
    "\n",
    "Text is usually represented as sequences of characters by computers. \n",
    "However, most natural language processing (NLP) and text mining tasks\n",
    "(e.g., parsing, information extraction, machine translation, document classification, information\n",
    "retrieval, etc.) need to operate on tokens. \n",
    "The process of breaking a stream of text into tokens is often referred to as **tokenization**.\n",
    "For example, a tokenizer turns a string such as \n",
    "```\n",
    "    A data wrangler is the person performing the wrangling tasks.\n",
    "```\n",
    "into a sequence of tokens such as\n",
    "```\n",
    "    \"A\" \"data\" \"wrangler\" \"is\" \"the\" \"person\" \"performing\" \"the\" \"wrangling\" \"tasks\"\n",
    "```\n",
    "\n",
    "There is no single right way to do tokenization. \n",
    "It completely depends on the corpus and the text analysis task you are going to perform. It is important to ensure that your tokenizer produces proper token types for your downstream text analysis tools. \n",
    "Although word tokenization is relatively easy compared with other NLP or text mining task, errors made in this phase will propagate into later analysis and cause problems.\n",
    "In this section, we will demonstrate the process of chopping character sequences into pieces with different tokenizers. \n",
    "\n",
    "The major question of the tokenization phase is what counts as a token.\n",
    "Different linguistic analyses might have different notions of tokens.\n",
    "In different languages, a token could mean different things. \n",
    "Here we are not going to dive into the linguistic aspect of what counts as a token,\n",
    "as it goes beyond the scope of this unit.\n",
    "We rather consider English text.\n",
    "**In English, a token can be a string of alphanumeric characters separated by spaces, which\n",
    "seems quite easy.**\n",
    "However, things get considerably worse when we start considering words having\n",
    "hyphens, apostrophes, periods and so on. In a word tokenization task, should we\n",
    "remove hyphens? Should we keep periods? \n",
    "According to different text analysis tasks, \n",
    "tokens can be unigram words, multi-word phrases (or collocations), or \n",
    "other meaningful and identifiable linguistic elements.\n",
    "Therefore, working out word tokens is not an easy task in pre-processing natural language text.\n",
    "You might be interested in watching a YouTube video on [word tokenization](https://www.youtube.com/watch?v=f9o514a-kuc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i7sE3bDa6a72"
   },
   "outputs": [],
   "source": [
    "raw = \"\"\"The GSO finace group in  U.S.A. provided Cole with about\n",
    "US$40,000,555.4 in funding, which accounts for 35.3% of Cole's revenue (i.e., AUD113.3m), \n",
    "as the ASX-listed firm battles for its survival.\n",
    "Mr. Johnson said GSO's recapitalisation meant \"the current shares are worthless\".\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r2yfyRDj6a8A"
   },
   "source": [
    "#### 2.1.1 Standard Tokenizer\n",
    "\n",
    "For English, a straightforward tokenization strategy is to use white spaces as token delimiters. \n",
    "The whitespace tokenizer simply splits the text on any sequence of whitespace, tab, or newline characters.\n",
    "Consider the above hypothetical text.\n",
    "As a starting point, let's tokenize the text above by using any whitespace characters as token delimiters.\n",
    "As mentioned, these characters include whitespace (' '), tab ('\\t'), newline ('\\n'), return ('\\r'), and so on.\n",
    "You have learnt in week 2 that those characters are together represented by a built-in regular expression abbreviation '\\s'.\n",
    "Thus, we will use '\\s' rather than writing it as something like '[ \\t\\n]+'.\n",
    "You can read the details about the [\"\\s\" Syntax](https://docs.python.org/3/library/re.html)\n",
    "\n",
    "There are multiple ways of tokenizing a string with whitespaces.\n",
    "The simplest approach might be using Python's string function `split()`.\n",
    "This function returns a list of tokens in the string.\n",
    "Another way is to use Python's regular expression package, `re` as\n",
    "```python\n",
    "    import re\n",
    "    re.split(r\"\\s+\", raw)\n",
    "```\n",
    "The output should be exactly the same as that given by the string function `split()`.\n",
    "Here we further demonstrate the use of <font color=\"blue\">RegexpTokenzier</font> from Natural Language Toolkit (NLTK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yKv-cbC96a8G"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QAWtbwiJ6a8R"
   },
   "outputs": [],
   "source": [
    "#For the RegexpTokenizer function, the arguement gaps type is bool.\n",
    "#we will use the 'True' if this tokenizer's pattern should be used to find separators between tokens; \n",
    "#we will use the 'False' if this tokenizer's pattern should be used to find the tokens themselves.\n",
    "\n",
    "#The below example with gasp param is True\n",
    "tokenizer = RegexpTokenizer(r\"\\s+\", gaps=True)\n",
    "tokens = tokenizer.tokenize(raw)\n",
    "print(tokens)\n",
    "\n",
    "#The below example with gasp param is False\n",
    "tokenizer_test = RegexpTokenizer(r\"\\s+\", gaps=False)\n",
    "tokens_test = tokenizer_test.tokenize(raw)\n",
    "print(tokens_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dvEgk0LU6a8e"
   },
   "source": [
    "A <font color=\"blue\">RegexpTokenizer</font> splits a string into tokens using a regular expression.\n",
    "Refer to its online [documentation](http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.regexp.RegexpTokenizer) \n",
    "for more details.\n",
    "Its constructor takes four arguments.\n",
    "The compulsory argument is the pattern used to build the tokenizer.\n",
    "It is in the form of a regular expression. \n",
    "**In the example above, we used `\\s+` to match 1 or more whitespace characters.**\n",
    "If the pattern defines separators between tokens, the value of `gaps` should be\n",
    "set to `True`. Otherwise, the pattern should be used to find the tokens.\n",
    "NLTK also provides a whitespace tokenizer, `WhitespaceTokenizer[source]`, which is\n",
    "equivalent to our tokenizer. Try\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "akOPDrnC-_H_"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "WhitespaceTokenizer().tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JVYonrOs-_xK"
   },
   "source": [
    "It seems that word tokenization is quite simple if words in a language are all\n",
    "separated by whitespace characters. \n",
    "However, this is not the case in many languages other than English, **such\n",
    "as Chinese, Japanese, Korean and Ancient Greek.** \n",
    "In those languages, text is written without any whitespaces between words. \n",
    "So the whitespace tokenizer is of no use at all.\n",
    "To handle them, we need more advanced tokenization techniques, often referred to as\n",
    "word segmentation, which is an important and challenging task in NLP. \n",
    "**However,\n",
    "discussing word segmentation is beyond our scope here.**\n",
    "\n",
    "It is not surprising that the whitespace tokenizer is **insufficient** even for English, since English does not just contains sequences of alphanumeric characters separated by white spaces. \n",
    "It often contains punctuation, hyphen, apostrophe, and so on.\n",
    "Sometimes **whitespace does not necessarily indicate a word break. **\n",
    "For example, non-compositional phrases (e.g., \"real estate\" and \"shooting pain\") and proper nouns (e.g., \"The New York Times\") have a different meaning than the sum of their parts. They cannot be split in the process of word tokenization.\n",
    "They must be treated as a whole in, for instance, information retrieval.\n",
    "\n",
    "Back to our example, \n",
    "the whitespace tokenizer still gives us word like \"(i.e.,\", \"funding,\" and \"worthless\".\".\n",
    "We would like to remove parentheses, some punctuations, quotation marks and other non-alphanumeric characters.\n",
    "A simple and straightforward strategy is to use all non-alphanumeric characters as token delimiters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rj9Uo8PG6a8k"
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r\"\\W+\", gaps=True) \n",
    "tokenizer.tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xDJ813sU6a82"
   },
   "source": [
    "In regular expressions, '\\W' indicates any non-alphanumeric characters (equivalent to `[^a-zA-Z0-9]`) while '\\w' indicates any alphanumeric characters (equivalent to `[a-zA-Z0-9]`). \n",
    "The counterpart is to extract tokens that only consist of alphanumeric characters without the empty strings. Try the following out yourself:\n",
    "```python\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    tokenizer.tokenize(raw)\n",
    "```\n",
    "\n",
    "These two strategies are simple to implement, but there are cases where they may not match the desired behaviour. \n",
    "For example, the whitespace tokenizer cannot properly handle non-alphanumeric characters, while the non-alphanumeric tokenizer might over-tokenise some tokens with periods, hyphens, apostrophes, etc.\n",
    "In the rest of this section, we will discuss the main problems that you might face while tokenising free language text. You will soon find that tokenizers should often be customized to deal with different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xaDUmnKoE59l"
   },
   "outputs": [],
   "source": [
    "#\\w means match any alphanumberic characters\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\", gaps=True) \n",
    "tokenizer.tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U_1ScDlL6a86"
   },
   "source": [
    "#### 2.1.2 Periods in Abbreviations\n",
    "\n",
    "Word tokens are not always surrounded by whitespace characters. Punctuation, such as commas, semicolons, and periods, are often used in English, as they are vital to disambiguate the meaning of sentences. However, it is problematic for computers to handle punctuation, especially periods, properly in tokenization. \n",
    "In this part we will focus on the handling of periods.\n",
    "\n",
    "Periods are usually used to mark the end of sentences. Difficulty arises when the period marks abbreviations (including acronyms). Please refer to **\"Step 2: Handling Abbreviations\" in [3]** for a detailed discussion on abbreviations.  In the case of abbreviations, particularly acronyms, separating tokens on punctuation and other non-alphanumeric characters would put different components of the acronym into different tokens, as you have seen in our example, where \"U.S.A\" has been put into three tokens, \"U\", \"S\" and \"A\", losing the meaning of the acronym. To deal with abbreviations, one approach is to maintain a look-up list of known abbreviations during tokenization. Another approach aims for smart tokenization. Here we will show you how to use regular expressions to cover most but not all abbreviations.\n",
    "\n",
    "An acronym is often formed from the initial components in multi-word phrases.  Some contains periods, and some do not. Common acronyms with periods are for example, \n",
    "* U.S.A\n",
    "* U.N.\n",
    "* U.K.\n",
    "* B.B.C\n",
    "\n",
    "Other abbreviations with a similar pattern are, for instance, \n",
    "* A.M. and P.M.\n",
    "* A.D. and B.C.\n",
    "* O.K.\n",
    "* i.e.\n",
    "* e.g.\n",
    "\n",
    "For abbreviations like those, it is not hard to figure out the pattern and the corresponding regular expression.  Each of those abbreviations contains at least a pair of a letter (either uppercase or lowercase) and a period.  The regular expression is\n",
    "```python\n",
    "    r\"([a-zA-Z]\\.)+\"\n",
    "```\n",
    "To see the graphical representation of the regular expression, please click the [RegexpTokenizer](https://regexper.com/#%28%5Ba-zA-z%5D%5C.%29%2B) webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GYOFGHKtG-uy"
   },
   "outputs": [],
   "source": [
    "#If you directly use the r\"([a-zA-Z])\", you will find out that the output is different with your expect. \n",
    "tokenizer = RegexpTokenizer(r\"([a-zA-Z]\\.)+\")\n",
    "tokenizer.tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xtdc9DPK6a8-"
   },
   "outputs": [],
   "source": [
    "#Then, we add the ?: in the above regular expression.\n",
    "tokenizer = RegexpTokenizer(r\"(?:[a-zA-Z]\\.)+\")\n",
    "tokenizer.tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YpjaA1Jm6a9I"
   },
   "source": [
    "Observe that\n",
    "1. We introduced <font color=\"red\">(?: )</font> in the regular expression to avoid just selecting substrings that match the pattern. `(?:)` is a non-capturing version of regular parentheses. If the parentheses are used to specify the scope of the pattern, but not to select the matched material to be output, you have to use `(?:)`. To check out how `?:` affects the output, try to remove it and run the tokenizer again. You will get the following output\n",
    "```\n",
    "    ['e.', 'A.', 'l.', 'r.']\n",
    "```\n",
    "It just returns the last substrings that match the pattern.\n",
    "2. The code also returned 'l.' and 'r.' that are part of 'survival.' and 'Mr.' \n",
    "The period in 'survival.' marks the end of a sentence. \n",
    "Indeed, it is very challenging to deal with the period at the end of each sentence, as it can also be part of an abbreviation if the abbreviation appears at the end of a sentence.\n",
    "For example, the following sentence ends with 'etc.'\n",
    "```\n",
    "    I need milk, eggs, bread, etc.\n",
    "```\n",
    "\n",
    "Next, let’s further consider some more general abbreviations, like\n",
    "* Mr. and Mrs.\n",
    "* Dr.\n",
    "* st.\n",
    "* Wash. and Calif. (abbreviations for two states in U.S., Washington and California)\n",
    "\n",
    "In those abbreviations, the period is always preceded two or more letters in English alphabet. Turn this pattern into a regular expression\n",
    "```\n",
    "    r\"[a-zA-z]{2,}\\.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_1gelRs56a9X"
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r\"[a-zA-z]{2,}\\.\")\n",
    "tokenizer.tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7U3-4wm26a9r"
   },
   "source": [
    "It is not surprising that the ouput contains \"survival.\" again. \n",
    "The issue of working out which punctuation marks indicate the end of a setence will be discussed in section 2.5.\n",
    "Let's put all the cases together. \n",
    "The regular expression can be generalised to\n",
    "```python\n",
    "    r\"([a-zA-Z]+\\.)+\"\n",
    "```\n",
    "which matches both acronyms and abbreviations like \"Dr.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gZjNqYKeJiZk"
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r\"([a-zA-z]+\\.)+\")\n",
    "tokenizer.tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jhUyz8cy6a92"
   },
   "source": [
    "As we mentioned early in this chapter, the issues of tokenization are language specific.\n",
    "The language of the document to be tokenized should be known a priori.\n",
    "Take computer technology as an example.\n",
    "It has introduced new types of character sequences that a tokenizer should probably treat as a single token, including email addresses, web URLs, IP addresses, etc. One solution is to simply ignore them by using a non-alphanumeric-based tokenizer. \n",
    "However, this comes the cost of losing the original meaning of those kinds of tokens. For instance, if an IP address, like \"172.19.197.106\", is tokenized into individual numbers, \"172\", \"19\", \"197\", and \"106\".\n",
    "It is no longer an IP address, and these numbers can be anything.\n",
    "To account for strings like\n",
    "* \"172.19.197.106\"\n",
    "* \"www.mit.edu\"\n",
    "\n",
    "you can simply update our regular expression accounting for abbreviations to \n",
    "```python\n",
    "    (\\w+\\.?)+\n",
    "```\n",
    "\n",
    "Try it out on http://regexr.com/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u08JXBR9J-22"
   },
   "outputs": [],
   "source": [
    "#Token for mathing IP address\n",
    "tokenizer = RegexpTokenizer(r\"\\d{1,3}\")\n",
    "print(tokenizer.tokenize(\"172.19.197.106\"))\n",
    "\n",
    "#Token for mathing word in a UTL\n",
    "tokenizer = RegexpTokenizer(r\"\\w{1,}\")\n",
    "print(tokenizer.tokenize(\"www.mit.edu\"))\n",
    "\n",
    "#the last word in a IP address or a URL\n",
    "tokenizer = RegexpTokenizer(r\"(\\w+\\.?)+\")\n",
    "print(tokenizer.tokenize(\"172.19.197.106\"))\n",
    "print(tokenizer.tokenize(\"www.mit.edu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uEujTvvy6a99"
   },
   "source": [
    "#### 2.1.3 Currency and Percentages\n",
    "\n",
    "While analysing financial document, such as finance reports, a financial analyst might be interested in monetary numerals mentioned in the reports. One interesting research question in both finance and computer science is whether one can use finance reports to help predict the stock market prices. In this case, it would be good for a tokenizer to keep all the monetary numerals.\n",
    "\n",
    "Currency is usually expressed in symbols and numerals (e.g., $10).\n",
    "There are many different ways of writing about different currencies.\n",
    "For example,\n",
    "* A three-letter currency abbreviations followed by figures, for example,\n",
    "```\n",
    "    AUD100, EUR500, CNY330 \n",
    "```\n",
    "\n",
    "* A letter or letters symbolising the country followed the, for example,\n",
    "```\n",
    "    A$100 (= AUD100), US$10 (= USD10), C$5 (= CAD5),\n",
    "```\n",
    "\n",
    "* A currency symbols ($, £, €, ¥, etc.) followed by figures, for examples\n",
    "```\n",
    "    £100.5, €30.0\n",
    "```\n",
    "\n",
    "While the number of digits in the integer part is more than three, commas are often inserted between every three digits, like\n",
    "```\n",
    "    AUD100, 000 \n",
    "```\n",
    "Let's construct a regular expression that can account for all the following monetary numerals\n",
    "```\n",
    "1. $10,000.00\n",
    "2. €10,000,000.00\n",
    "3. ¥5.5555\n",
    "4. AUD100\n",
    "5. A$10.555\n",
    "```\n",
    "The regular expression should looks like as follows (<a href=\"https://regexper.com/#(%3F%3A%5BA-Z%5D%7B1%2C3%7D)%3F%5B%5C%24£€¥%5D%3F(%3F%3A%5Cd%7B1%2C3%7D%2C)*%5Cd%7B1%2C3%7D(%3F%3A%5C.%5Cd%2B)%3F\"> the graphical representation</a>):\n",
    "```python\n",
    "    r\" (?:          \n",
    "        [A-Z]{1,3})?                 # (1)\n",
    "        [\\$£€¥]?         # (2)\n",
    "        (?:\\d{1,3},)*      # (3)\n",
    "        \\d{1,3}          # (4)\n",
    "        (?:\\.\\d+)?        # (5)\n",
    "   \"\n",
    "```\n",
    "\n",
    "![The diagram for this regular expression](https://github.com/tulip-lab/sit742/raw/master/Jupyter/image/P04A01.png)\n",
    "\n",
    "\n",
    "(1) matches the start of monetary numerals, which consists of one or up to 3 uppercase letters that indicate a country symbol or a currency abbreviation.\n",
    "<br/>\n",
    "(2) together with (1), matches the start of monetary numerals, which consists of either only a currency symbol or a country symbol plus a currency symbol.\n",
    "<br/>\n",
    "(3) accounts for the integer part that contains more than three digits. It matches all digits in the integer part except for the last three digits.\n",
    "<br/>\n",
    "(4) matches the last three digits in the integer part.\n",
    "<br/>\n",
    "(5) matches the fractional part.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Drp_VOFm6a-T"
   },
   "outputs": [],
   "source": [
    "#Let run the above regular expression\n",
    "tokenizer = RegexpTokenizer(r\"(?:[A-Z]{1,3})?[\\$£€¥]?(?:\\d{1,3},)*\\d{1,3}(?:\\.\\d+)?\")\n",
    "tokenizer.tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uXanOxsH6a-n"
   },
   "source": [
    "Refer back to our example text \"raw\", can you find any issue rather than the percentage (35.5%)? The regular expression cannot handle \"AUD113.3m\", where the \"m\" indicates million. Without 'm', the number 'AUD113.3' loses its meaning in the original context. Therefore, you have seen that there might not be a regular expression that can handle all possible ways of representing currency.\n",
    "\n",
    "Now, we have constructed a regular expression for currencies, even though it is not perfect.\n",
    "Next, we move to working out the regular expression for percentages, things becomes quite easy.\n",
    "Percentages usually have the following forms\n",
    "* 23%\n",
    "* 23.23%\n",
    "* 23.2323%\n",
    "* 100.00%\n",
    "\n",
    "The maximum number of digits in the integer part is 3, the minimun is 1, so the regular expression is '\\d{1,3}'.\n",
    "A percentage can have either one or no fractional part, which can be matched by '(\\.\\d+)?'.\n",
    "Adding % to the end, we have (<a href=\"https://regexper.com/#%5Cd%7B1%2C3%7D(%5C.%5Cd%2B)%25\">the graphical representation</a>)\n",
    "```python\n",
    "    r\"\\d{1,3}(\\.\\d+)%\"\n",
    "```\n",
    "\n",
    "![The diagram for this regular expression](https://github.com/tulip-lab/sit742/raw/master/Jupyter/image/P04A02.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I45vt11t6a-s"
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r\"\\d{1,3}(?:\\.\\d+)?%\")\n",
    "tokenizer.tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NsfJgax26a-2"
   },
   "source": [
    "The above code should give you the only percentage in our example text. \n",
    "Compare the regular expression matching percentages with that matching currency,\n",
    "you will find that the former is similar to the last bits of the latter, except for the percentage sign.\n",
    "Besides, there are other numerical and special expressions that\n",
    "we can not easily handle with regular expressions. For example, these expressions include\n",
    "email addresses, time, vehicle licence numbers, phone numbers, etc.\n",
    "If you are interested in dealing with them, you could read the “Regular Expressions Cookbook” by Jan Goyvaerts and Steven Levithan. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hngoWmJi6a-5"
   },
   "source": [
    "#### 2.1.4 Hyphens and Apostrophes \n",
    "\n",
    "In English, hyphenation is used for various purposes. The hyphen can be used to form certain compound terms, including hyphenated compound nouns, verbs and adjectives. It can also be used for word division. There are many sources of hyphens in texts. Thus, should one count a sequence of letters with a hyphen as one word to two? Unfortunately, the answer seems to be sometimes one, sometimes two. \n",
    "For example, if the hyphen is used to split up vowels in words, such as \"co-operate\", \"co-education\" and \"pre-process\", these words should be regarded as single token. In contrast, if the hyphen is used to group a couple of words together, for example, \"a state-of-the-art algorithm\" and \"a money-back guarantee\", these hyphenated words should be separated into individual words.\n",
    "Therefore, handling hyphenated words automatically is one of the most difficult tasks in pre-processing text data.\n",
    "\n",
    "\"**The Art of Tokenization**\" (Please refer the Part 4, Reading Materials) categorizes different hyphens into three types:\n",
    "* **End-of-Line Hyphen**: In professionally printed material (like books, and newspapers), the hyphen is used to divide words between the end of one line and the beginning of the next in order to perform justification of text during typesetting. It seems to be easy to handle these kinds of hyphens by simply removing them and joining the parts of a word at the end of one line and the beginning of the next.\n",
    "* **Lexical Hyphen**: Words with a lexical hyphen are better to be treated as a single word. They are typically included in a dictionary. For example, words contains certain prefixes, like \"co-\", \"pre-\", \"multi-\", etc., and other words like \"so-called\", \"forty-two\"\n",
    "* **Sententially Determined Hyphenation**: This type of hyphen is often created dynamically. It includes, for example, nouns modified by an 'ed'-verb (e.g., \"text-based\" and \"hand-made\") and sequences of words used as a modifier in a noun group, as in \"the 50-cent-an-hour raise\". In these cases, we might want to treat those tokens joined by hyphens as individual words.\n",
    "\n",
    "The use of hyphens in many such cases is extremely inconsistent, which further increase the complexity of dealing with hyphens in tokenization. People often resort to using either some heuristic rules or treating it as a machine learning problem. However, these go beyond our scope here. It is clear that handling hyphenation is much more complicated than one can expect. You should also be clear that there is no way of handling all the cases above.\n",
    "\n",
    "Let's assume that we are going to treat all strings of two words separated by a hyphen as a single token, how can we extract them from texts without breaking them into pieces.  In our example text, we are going to view \"ASX-listed\" as a single token. The pattern here is  a sequence of alphanumeric character plus \"-\" and plus another sequence of alphanumeric character.\n",
    "The corresponding regular expressions should be \n",
    "```python\n",
    "    r\"\\w+-\\w\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I9rchg8I6a-_"
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r\"\\w+-\\w+\")\n",
    "tokenizer.tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FKnqjBWh6a_K"
   },
   "source": [
    "Similar to hyphens, how to handle an apostrophe in tokenization is another complex question. The apostrophe in English is often used in two cases:\n",
    "* Contractions: a shortened version of a word or multiple words. \n",
    "    * don't (do not)\n",
    "    * she'll (she will)\n",
    "    * you're (you are)\n",
    "    * he's (he is or he has)\n",
    "    * you'd (you would)\n",
    "* Possessives: used to indicate ownership/possession with nouns.\n",
    "    * the cat's tail\n",
    "    * Einstein's theory\n",
    "    \n",
    "Should we treat a string containing apostrophes as a single word or two words?\n",
    "Perhaps, you might think we should separate English Contractions into two words, and regard possessives as a single word. \n",
    "However, distinguishing contractions from possessives is not easy.\n",
    "For example, should \"cat's\" be \"cat has/is\" or the possessive case of cat.\n",
    "Thus some processor in NLP splits the strings in either case into two words, while others do not.\n",
    "Here we again assume that we are going to retrieve all strings with an apostrophe as single words.\n",
    "The regular expression is quite similar to the one for handling hyphens.\n",
    "```\n",
    "     r\"\\w+'\\w+\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p8lUk62W6a_Y"
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r\"\\w+'\\w+\")\n",
    "tokenizer.tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e5trx9fP6a_x"
   },
   "source": [
    "Now let's generalise the `\\w+` to permit word-internal hyphens and apostrophes (<a href=\"https://regexper.com/#%5Cw%2B(%3F%3A%5B-'%5D%5Cw%2B)%3F\">the graphical representation</a>):\n",
    "```python\n",
    "    \\w+(?:[-']\\w+)? \n",
    "```\n",
    "\n",
    "You have learnt some simple approaches for handling different issues in word tokenization, which turns out to be far more difficult than you might have expected. It is clear that different NLP and text mining tasks on different text corpora need different word tokenization strategies, as you must decide what counts as a word. Besides the `RegexpTokenizer`, NLTK implements a set of other word tokenizaton modules. Please refer to [its official webpage](http://www.nltk.org/api/nltk.tokenize.html) for more details.\n",
    "So far that we have only considered well-written text, but there are other types of natural language texts, such the transcripts of speech corpora and some non-standard texts like tweets that provide their own additional challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lnhfVL6HR6Hz"
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r\"\\w+(?:[-']\\w+)? \")\n",
    "tokenizer.tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tp9IPKDr6a_5"
   },
   "source": [
    "### 2.2. Case Normalization\n",
    "After word tokenization, you may find that words can contain either upper- or lowercase letters. \n",
    "For example, you might have \"data\" and \"Data\" appearing in the same text.\n",
    "Should one treat them as two different words or as the same word?\n",
    "Most English texts are written in mixed case. \n",
    "In other words, a text can contain both upper- and lowercase letters.\n",
    "Capitalization helps readers differentiate, for example, between nouns and proper nouns.\n",
    "In many circumstances, however, an uppercase word should be treated no differently than in lower case appearing in a document, and even in a corpus.\n",
    "Therefore, a common strategy is to reduce all letters in a word to lower case.\n",
    "It is very simple to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H-BhfNO36a__"
   },
   "outputs": [],
   "source": [
    "tokens = [token.lower() for token in tokens]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R4hQJMHE6bAP"
   },
   "source": [
    "It is often a good idea to do case normalization. For example, with case normalization, you can match \"data wrangling\" with \"Data Wrangling\" in an information retrieval task. But for other tasks, like named entity recognition, one would better to keep capitalised words (e.g., pronouns) left as capitalised.\n",
    "People have tried some simple heuristics that just makes some token lowercase. \n",
    "However, there is a trade-off between getting capitalization right and simply using lowercase regardless of the correct case of words.\n",
    "You can read about basic formatting issues of text processing in \"Corpus-Based Work\" on the Part 4, Reading Materials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gWw6xdFV6bAY"
   },
   "source": [
    "### 2.3. Removing Stop words\n",
    "[Stopwords](https://en.wikipedia.org/wiki/Stop_words) are words that are extremely common and carry little lexical content. For many NLP and text mining tasks, it is useful to remove stopwords in order to save storage space \n",
    "and speed up processing, and the process of removing these words is usually called “stopping.” \n",
    "An example stopword list from NLTK is shown bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bQc14Mrg6bAx"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "#show the stopword in the 'english' database\n",
    "stopwords_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fdELVGtT6bBR"
   },
   "source": [
    "The above list contains 127 stopwords in total, which are often [function words](https://en.wikipedia.org/wiki/Function_word) in English, like articles (e.g., \"a\", \"the\", and \"an\"), \n",
    "pronouns (e.g., \"he\", \"him\", and \"they\"), particles (e.g., \"well\", \"however\" and \"thus\"), etc.\n",
    "It is easy to use NLTK's built-in stopword list to remove all the stopwords from a tokenised text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vOM8t8d_6bBl"
   },
   "outputs": [],
   "source": [
    "filtered_tokens = [token for token in tokens if token not in stopwords_list]\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8QTQwnCdTvct"
   },
   "outputs": [],
   "source": [
    "#This will show all exclude stopwords from the filtered list\n",
    "excluded_tokens = [token for token in tokens if token in stopwords_list]\n",
    "excluded_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tiG652Wi6bB2"
   },
   "source": [
    "We have removed 13 stopwords. The rest Token number is 28. \n",
    "To check what stopwords have been excluded from the filtered list, you simply change `not in` to `in`.\n",
    "\n",
    "There is no single universal list of stop words used by all NLP and text mining tools.\n",
    "Different stopword lists are available online. For example, the English stopword list \n",
    "available at [Kevin Bouge's website](https://sites.google.com/site/kevinbouge/stopwords-lists) \n",
    "which contains 570 stopwords, a quite fine-grained stopword list. \n",
    "At the same website, you can also download stopword lists for 27 languages other than English.\n",
    "Please download the English stopwords list from Kevin Bourge's website, and save it into the folder where\n",
    "you keep this IPython Notebook file. \n",
    "We will try out the aforementioned stopword lists on the large\n",
    "[Reuters corpus](https://github.com/teropa/nlp/tree/master/resources/corpora/reuters). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "99wYIR5x6bB8"
   },
   "outputs": [],
   "source": [
    "!pip install wget\n",
    "\n",
    "import wget\n",
    "\n",
    "link_to_data = 'https://github.com/tulip-lab/sit742/raw/master/Jupyter/data/stopwords_en.txt'\n",
    "\n",
    "DataSet = wget.download(link_to_data)\n",
    "\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4KaWJx9R6bCw"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "reuters = nltk.corpus.reuters.words()\n",
    "\n",
    "stopwords_list_570 = []\n",
    "with open('stopwords_en.txt') as f:\n",
    "    stopwords_list_570 = f.read().splitlines()\n",
    "#It will show the retuers stopwords, you can compare it with the above 'english'stopwords.\n",
    "#You will find that the 'retuers'stopwords is more abundant than 'english' stopwords. \n",
    "stopwords_list_570"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ixs2WLXN6bDn"
   },
   "source": [
    "Remove stop words accroding to NLTK's built-in stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sp4I6HPH6bEB"
   },
   "outputs": [],
   "source": [
    "filtered_reutuers = [w for w in reuters if w.lower() not in stopwords_list]\n",
    "#It will show the percentage between the filtered_retuers and the 'english' stopwords\n",
    "len(filtered_reutuers)*1.0/len(reuters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QPqNXCo6bEf"
   },
   "source": [
    "Remove stop words according to the downloaded stop word list. (Note: the following script will run a couple of minutes due to data structure used in search.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CroshJn16bEk"
   },
   "outputs": [],
   "source": [
    "filtered_reutuers = [w for w in reuters if w.lower() not in stopwords_list_570]\n",
    "#It will show the percentage between the filtered_retuers and the 'retuers' stopwords\n",
    "#It will show that the retuers stopwords will filte more stopwords. \n",
    "len(filtered_reutuers)*1.0/len(reuters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vbc9EF4j6bEx"
   },
   "source": [
    "Thus, with the help of these two stopword lists, we can filter about 36% and 34% of the words respectively.\n",
    "We have significantly reduced the size of the Reuters corpus. \n",
    "The question is: Have we lost lots of information due to removing stopwords? \n",
    "For the large majority of NLP and text mining tasks and algorithms, stopwords usually appear to be of little value and have little impact on the final results, as the presence of stopwords in a text does not really help distinguishing it from other texts. \n",
    "In contrast, text analysis tasks involving phrases are the exception because phrases lose their meaning if some of the words are removed. \n",
    "For example, if the two stopwords in the phrase \"a bed of roses\" are removed, its original meaning in the context of IR will be lost.\n",
    "\n",
    "Stopwords usually refer to the most common words in a language. \n",
    "The general strategy for determining whether a word is a stopword or not is to compute its total number of appearances in a corpus. \n",
    "We will cover more about removing common words other than stopwords while we further explore text data in next chapter.\n",
    "Here we would like to point out that failing to remove those common words could lead to skewed analysis results.\n",
    "For example, while analysing emails we usually remove headers (e.g., \"Subject\", \"To\", and \"From\") and sometimes\n",
    "a lengthy legal disclaimer that often appears in many corporate emails.\n",
    "For short messages, a long disclaimer can overwhelm the actual text when performing any sort of text analysis.\n",
    "For more discussion on stopping, please read [5] and watch an 8-mintue YouTube video on [Stop Words](https://www.youtube.com/watch?v=w36-U-ccajM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SKZTPma-6bEz"
   },
   "source": [
    "### 2.4. Stemming and Lemmatization\n",
    "\n",
    "Another question in text pre-processing is whether we want to keep word forms like \"educate\", \"educated\", \"educating\", \n",
    "and \"educates\" separate or to collapse them. Grouping such forms together and working in terms of their base form is \n",
    "usually known as stemming or lemmatization.\n",
    "Typically the stemming process includes the identification and removal of prefixes, suffixes, and pluralisation, \n",
    "and leaves you with a stem.\n",
    "Lemmatization is a more advanced form of stemming that makes use of, for example, the context surrounding the words, \n",
    "an existing vocabulary, morphological analysis of words and other grammatical information (e.g., part-of-speech tags) \n",
    "to determine the basic or dictionary form of a word, which is known as the lemma.\n",
    "See Wikipedia entries for [stemming](https://en.wikipedia.org/wiki/Stemming) \n",
    "and [lemmatization](https://en.wikipedia.org/wiki/Lemmatisation).\n",
    "\n",
    "Stemming and lemmatization are the basic text pre-processing methods for texts in languages like English, French, \n",
    "German, etc. \n",
    "In English, nouns are inflected in the plural, verbs are inflected in the various tenses, and adjectives are \n",
    "inflected in the comparative/superlative. \n",
    "For example,\n",
    "* watch &#8594; watches\n",
    "* party &#8594; parties\n",
    "* carry &#8594; carrying\n",
    "* love &#8594; loving\n",
    "* stop &#8594; stopped\n",
    "* wet &#8594; wetter\n",
    "* fat &#8594; fattest\n",
    "* die &#8594; dying\n",
    "* meet &#8594; meeting\n",
    "\n",
    "It is not hard to find that they all follow some inflections rules. \n",
    "For instance, to get the plural forms of nouns endings with consonant 'y', one often changes the ending \n",
    "'y' to 'ie' before adding 's'. \n",
    "Indeed most existing stemming algorithms make intensive use of this kind of rules.\n",
    "\n",
    "In morphology, the derivation process creates a new word out of an existing one often by adding either \n",
    "a prefix or a suffix. It brings considerable sematic changes to the word, often word class is changed, for example,\n",
    "* dark &#8594; darkness\n",
    "* agree &#8594; agreement\n",
    "* friend &#8594; friendship\n",
    "* derivation &#8594; derivational\n",
    "\n",
    "The goal of stemming and lemmatization is to reduce either inflectional forms or derivational forms of \n",
    "a word to a common base form. \n",
    "Before we demonstrate the use of several state-of-the-art stemmers and lemmatizers implemented in NLTK, please read\n",
    "[4] and section 3.6 in [2].\n",
    "If you are a visual learner, you could watch the YouTube video on \n",
    "[Stemming](https://www.youtube.com/watch?v=2s7f8mBwnko) from Prof. Dan Jurafsky.\n",
    "\n",
    "NLTK provides several famous stemmers interfaces, such as\n",
    "\n",
    "* Porter Stemmer, which is based on \n",
    "[The Porter Stemming Algorithm](http://tartarus.org/martin/PorterStemmer/)\n",
    "* Lancaster Stemmer, which is based on \n",
    "[The Lancaster Stemming Algorithm](https://tartarus.org/martin/PorterStemmer/),\n",
    "* Snowball Stemmer, which is based on [the Snowball Stemming Algorithm](http://snowball.tartarus.org/)\n",
    "\n",
    "Let's try the three stemmers on the words listed above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ukMvAMG46bFG"
   },
   "outputs": [],
   "source": [
    "words = ['watches', 'parties', 'carrying', 'loving', 'stopped', 'wetter', 'fattest', \n",
    "          'dying', 'darkness', 'agreement', 'friendship', 'derivational', 'denied',  'meeting']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BgaIIrNu6bFW"
   },
   "source": [
    "Porter Stemming Algorithm is the one of the most common stemming algorithms.\n",
    "It makes use of a series of heuristic replacement rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dgNgyugE6bFZ"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "['{0} -> {1}'.format(w, stemmer.stem(w)) for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5glx9qR76bFj"
   },
   "source": [
    "The Porter Stemmer works quite well on general cases, like 'watches' &#8594; 'watch' and 'darkness' &#8594; 'dark'.\n",
    "However, for some special cases, the Porter Stemmer might not work as expected, \n",
    "like  'carrying'  &#8594; 'carri' and 'derivational' &#8594; 'deriv'. \n",
    "Note that a concept called \"list comprehension\" supported by Python is used here.\n",
    "If you would like to know more about list comprehension, please click [here](http://www.secnetix.de/olli/Python/list_comprehensions.hawk).\n",
    "\n",
    "The Lancaster Stemmer is much newer than the Porter Stemmer, published in 1990."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w6tqQde_6bFo"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "['{0} -> {1}'.format(w, stemmer.stem(w)) for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jo6VCDD46bF9"
   },
   "source": [
    "After comparing the output from the Lancaster Stemmer and that from the Porter Stemmer, you might think that\n",
    "the Lancaster Stemmer could be a bit more aggressive than the Porter Stemmer, since it gets 'agreement' &#8594; 'agr' and 'derivational' &#8594; 'der'. \n",
    "At the same time, it seems that the Lancaster Stemmer can handle words like 'parties' and 'carrying' quite well.\n",
    "\n",
    "Now let's try the Snowball Stemmer.\n",
    "The version in NLTK is available in 15 languages.\n",
    "Different from the previous two stemmers, you need to specify which language the Snowball Stemmer will be applied to in its class constructor.\n",
    "It works in a similar way to the Porter Stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FBV_FV7y6bGR"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "['{0} -> {1}'.format(w, stemmer.stem(w)) for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hpZc2ZM26bGa"
   },
   "source": [
    "A stemmer usually resorts to language-specific rules. \n",
    "Different stemmers implementing different rules and behave differently, \n",
    "as shown above.\n",
    "The use of inflection and derivation is very complex in English.\n",
    "There might not exist a set of rules that can cover all the cases.\n",
    "Therefore, the stemmers that you have played will always generate some out-of-vocabulary words.\n",
    "\n",
    "Rather than using a stemmer, you can use a lemmatizer that utilises\n",
    "more information about the language to accurately identify the lemma\n",
    "for each word.\n",
    "As pointed out in \"**Stemming and lemmatization**\" (Please read the   related Reading Materials on the Part 4), \n",
    "> Stemmers use language-specific rules, but they require less knowledge than a lemmatizer, which needs a complete vocabulary and morphological analysis to correctly lemmatize words\n",
    "\n",
    "The WordNet lemmatizer implemented in NLTK is based on WordNet's built-in morphologic function, and returns the input word unchanged if it cannot be found in WordNet, which sounds more reasonable\n",
    "than just chopping off prefixes and suffixes. In NLTK, you can use it in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FIGTEBI16bGf"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "['{0} -> {1}'.format(w, lemmatizer.lemmatize(w)) for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kzbZuj8G6bG5"
   },
   "source": [
    "It is a bit strange that the lemmatizer did nothing to nearly all the words, except for 'watches', 'parties'\n",
    "However, if we specify the POS tag of each word, what will happen?\n",
    "Let try a couple of words in our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gCUQdQ7k6bHE"
   },
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize('dying', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fkb5sN6g6bHd"
   },
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize('meeting', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sEtrMIGt6bIK"
   },
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize('meeting', pos='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SFbhdbxD6bId"
   },
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize('wetter', pos='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_54mOzQc6bIl"
   },
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize('fattest', pos='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RJl_Da4Z6bIz"
   },
   "source": [
    "If we know the POS tags of the words, the WordNet Lemmatizer can accurately identify the corresponding lemmas.\n",
    "For example, the word 'meeting' with different POS tag, the WordNet Lemmatizer gives you different lemmas.\n",
    "Without giving the POS tags, it uses noun as default.\n",
    "\n",
    "Both stemming and lemmatization can significantly reduce the number of words in a vocabulary.\n",
    "In other words, the downstream text analysis tools can benefit from them by saving running time\n",
    "and memory space. In contrast, can stemming and lemmatization improve the performance\n",
    "of those tools? It is a quite arguable question. \n",
    "As pointed out in [4], stemming and lemmatization can increase recall but harm precision in information\n",
    "retrieval. Researchers have also found that classifying English document tasks often do not gain \n",
    "from stemming and lemmatization.\n",
    "However, it might not be the case when we change our language to something rather than English, for example, German."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Pfsl5v46bI6"
   },
   "source": [
    "### 2.5. Sentence Segmentation\n",
    "\n",
    "Sentence segmentation is also known as sentence boundary disambiguation or sentence boundary detection.\n",
    "The following is the Wikipedia definition of sentence boundary disambiguation:\n",
    ">Sentence boundary disambiguation (SBD), also known as sentence breaking, is the problem in natural language processing of deciding where sentences begin and end. Often natural language processing tools require their input to be divided into sentences for a number of reasons. However sentence boundary identification is challenging because punctuation marks are often ambiguous. For example, a period may denote an abbreviation, decimal point, an ellipsis, or an email address - not the end of a sentence. About 47% of the periods in the Wall Street Journal corpus denote abbreviations. As well, question marks and exclamation marks may appear in embedded quotations, emoticons, computer code, and slang.\n",
    "\n",
    "SBD is one of the essential problems for many NLP tasks, like Parsing, Information Extraction, Machine Translation, and Document Summarizations. \n",
    "The accuracy of the SBD system will directly affect the performance of these applications. \n",
    "\n",
    "Sentences are the basic textual unit immediately above the word and phrase. \n",
    "So what is a sentence? Is something ending with one of the following punctuations \".\", \"!\", \"?\"?\n",
    "Does a period always indicate sentence boundaries?\n",
    "For English texts, it is almost as easy as finding every occurrence of those punctuations.\n",
    "However, some periods occur as part of abbreviations, monetary numerals and percentages, as we \n",
    "have discussed in sections 1.2 and 1.3. \n",
    "Although you can use a few heuristic rules to correctly\n",
    "identify the majority of sentence boundaries, SBD is much more complex that we can expect,\n",
    "please read section 4.2.4 of the book, 'Corpus-Based Work'  refered on the Part 4 Reading Materials, and watch a Youtube video on [Sentence segmentation](https://www.youtube.com/watch?v=9LXq3oQEEIA). \n",
    "discussing more advanced techniques for SBD goes beyond our scope.\n",
    "Instead, we will show you some sentence segmentation tools implemented in NLTK.\n",
    "Please also note that there are other tools or packages containing a sentence tokenizer,\n",
    "for example, Apache OpenNLP, Stanford NLP toolkit, and so on.\n",
    "\n",
    "The NLTK's [Punkt Sentence Tokenizer](http://www.nltk.org/api/nltk.tokenize.html) was designed to split \n",
    "text into sentences \"*by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences.*” It contains a pre-trained sentence tokenizer for English.\n",
    "Let's test it out with a couple of examples extracted from the book, called \"Moby Dick\", on Project Gutenberg, by \n",
    "Herman Melville.\n",
    "First construct a pre-trained English sentence tokenizer,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N0JP3rNx6bJD"
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gbC3vCn76bJQ"
   },
   "source": [
    "Following the intruction on the official website of Punkt Sentence Tokenizer, we tokenize two snippets extracted\n",
    "from \"Moby Dick\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sPakoHAG6bJY"
   },
   "outputs": [],
   "source": [
    "text1 = '''And so it turned out; Mr. Hosea Hussey being from home, but leaving \n",
    "Mrs. Hussey entirely competent to attend to all his affairs. Upon making known our desires \n",
    "for a supper and a bed, Mrs. Hussey, postponing further scolding for the present, ushered us \n",
    "into a little room, and seating us at a table spread with the relics of a recently concluded repast, \n",
    "turned round to us and said—\"Clam or Cod?\"'''\n",
    "\n",
    "\n",
    "#('\\n-----\\n' is used to wrap the sentences after the stripped results, it is useful for\n",
    "#reading the processed the text)\n",
    "print('\\n-----\\n'.join(sent_detector.tokenize(text1.strip())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Gpu_ug56bJh"
   },
   "outputs": [],
   "source": [
    "text2 = '''A clam for supper? a cold clam; is THAT what you mean, Mrs. Hussey?\" says I, \"but\n",
    "that's a rather cold and clammy reception in the winter time, ain't it, Mrs. Hussey?\"'''\n",
    "print('\\n-----\\n'.join(sent_detector.tokenize(text2.strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9SoAsA-26bJ0"
   },
   "source": [
    "You can also use `sent_tokenize`, an instance of Punkt Sentence Tokenizer.\n",
    "This instance has already been trained on and works well for many European languages.\n",
    "```python\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    sent_tokenize(text1)\n",
    "```\n",
    "You should get similar outputs as above.\n",
    "\n",
    "Comparing the two results we notice that the sentence tokenizer has troubles in recognizing abbreviations.\n",
    "It got \"Mrs.\" right in the first snippet but not the second. Regarding this type of issues, please read a blog post on sentence tokenizer , just click the 'Testing out the NLTK sentence tokenizer' on the Part 4 Reading Materials. \n",
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cqJkbYlm6bJ3"
   },
   "source": [
    "## Part 3. Sumary\n",
    "\n",
    "In this chapter we have covered the fundamentals of text pre-processing. \n",
    "You have learnt how to access different text data, and how to carry out \n",
    "the following basic text pre-processing steps:\n",
    "* Tokenization\n",
    "* Case normalization\n",
    "* Stopping\n",
    "* Stemming and lemmatization\n",
    "* Sentence segmentation\n",
    "\n",
    "Now you should be able to perform those pre-processing tasks on a new corpus according\n",
    "to requirements of different text analysis tasks. \n",
    "We would like to point out that besides NLTK, there are other NLP tools with mixed quality, which can be used to process text data. For example, [the standford NLP group](http://nlp.stanford.edu/software/) provides a list of tools for parsing, POS tagging, Name Entity Regonition  (NER), word segmentation, tokinization, etc; \n",
    "and [Mallet](http://mallet.cs.umass.edu/) is a Java-based package for statistical natural langage processing. \n",
    "* * *\n",
    "\n",
    "## Part 4. Reading Materials\n",
    "\n",
    "1. \"[Tokenization](http://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html)\" 📖 .\n",
    "2. \"[Processing Row Text](http://www.nltk.org/book_1ed/ch03.html)\", chapter 3 of\n",
    "of \"Natural Language Processing with Python\".\n",
    "3. \"[The Art of Tokenization](https://www.ibm.com/developerworks/community/blogs/nlp/entry/tokenization?lang=en)\": An IBM blog on tokenization. It gives a detailed discussion about word tokenization and its challenges 📖 .\n",
    "4. \"[Stemming and lemmatization](http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)\" 📖 .\n",
    "5. \"[Dropping common terms: stop words](http://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html)\" 📖 .\n",
    "6. \"[Corpus-Based Work](https://www.deakin.edu.au/library)\", Chapter 4 of \"Foundations of statistical natural language processing\" by Christopher D. Manning 📖 .\n",
    "7. \"[Testing out the NLTK sentence tokenizer](http://www.robincamille.com/2012-02-18-nltk-sentence-tokenizer/)\"\n",
    "1. \"[Accessing Text Corpora and Lexical Resources](http://www.nltk.org/book/ch02.html): Chapter 2 of \"Natural Language Processing with Python\" By Steven Bird, Ewan Kelin & Edward Loper 📖 .\n",
    "2. \"[Corpus Readers](http://www.nltk.org/howto/corpus.html#tagged-corpora)\": An NLTK tutorial on accessing the contents of a diverse set of corpora.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "SIT742P04A-TextPreprocessing.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/tulip-lab/sit742/blob/master/Jupyter/SIT742P04A-TextPreprocessing.ipynb",
     "timestamp": 1552612501417
    }
   ],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
