{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# SIT742: Modern Data Science \n**(Week 04: Text Analysis)**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are free to use, change and distribute this package.\n- If you found any issue/bug for this document, please submit an issue at [tulip-lab/sit742](https://github.com/tulip-lab/sit742/issues)\n\nPrepared by **SIT742 Teaching Team**\n\n---\n\n## Session 4B - Exploring Pre-Processed text and Generating Features\n\n### Table of Content\n\n* Part 1. Counting Vocabulary by Selecting Tokens of Interest\n* Part 2. Building Vector Representation \n* Part 3. Saving Pre-processed Text to a File\n* Part 4. Extracting Other Features\n* Part 5. Summary\n* Part 6. Reading Materials\n* Part 7. Exercises\n\n\n---\n\nOne of the challenges of text analysis is to convert unstructured and semi-structured text into a structured representation. This must be done prior to carrying out any text analysis tasks. This chapter will show you \nhow to put some of those basic steps discussed in the previous chapter together to generate different vector\nrepresentations for some given text. You will learn how to compute some basic statistics for text, and how to extract features rather than unigrams.\n\n\n## Part 1. Counting Vocabulary by Selecting Tokens of Interest\n\nTwo important concepts that should be mentioned first are **type** and **token**.\nHere are the definitions of the two terms, quoted from \"[tokenization](http://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html)\",  \n>a **token** is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing;\n\n> a **type** is the class of all tokens containing the same character sequence. \n\nA *type* is also a vocabulary entry. In other words, a vocabulary consists of a number of word types.\nThe distinction between a type and its tokens is a distinction that separates a descriptive concept from\nits particular concrete instances. \nThis is quite similar to the distinction in object-oriented programming between classes and objects.\nIn this section, you are going to learn how to count types in a given corpus by further processing the text.\n\nThe document collection that we are going to use is a set of Reuters articles that comes with NLTK.\nIt contains 10788 Reuters articles in total and has been split into two subsets, training and testing.\nAlthough this collection has already been pre-processed (e.g., you can access the text at different levels, like raw text, tokens, and sentences),\nwe would still like to demonstrate how to put some of the basis text preprocessing steps together and process the raw Reuters articles step by step.\nFirst, import the main Python libraries.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import matplotlib.pyplot as plt\n%matplotlib inline \nimport nltk\nfrom nltk.corpus import reuters", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Since the tokenizer works on a per document level, we can parallelize the process of tokenization with Python's multi-processing module. Please refer to its official documentation [here](https://docs.python.org/2/library/multiprocessing.html).\nIn the following code, we wrap tokenization in a Python function, and then\ncreate a pool of four worker processes with the Python Pool class.\nThe <font color=\"blue\">Pool.map()</font>, a parallel equivalent of the  built-in  <font color=\"blue\">map()</font> function, takes one iterable argument.\nThe iterable will be split into a number of chunks, each of which will be submitted to a process in the process pool.\nEach process will apply a callable function to each element in the chunk it has received.\nNote that you can replace the NLTK tokenizer with the one you implement.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "def tokenizeRawData(fileid):\n    \"\"\"\n        This function tokenizes a raw text document.\n    \"\"\"\n    raw_article = reuters.raw(fileid).lower() # cover all words to lowercase\n    tokenised_article = nltk.tokenize.word_tokenize(raw_article) # tokenize each Reuters articles\n    return (fileid, tokenised_article) ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "nltk.download('reuters')\ntokenized_reuters =  dict(tokenizeRawData(fileid) for fileid in reuters.fileids())", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "### 1.1. Removing Words with Non-alphabetic Characters\nThe NLTK's built-in  <font color=\"blue\">word_tokenize</font> function tokenizes a string to split off punctuation other than periods.\nNot only does it return words with alphanumerical characters, but also punctuations. \nLet's take a look at one Reuters articles,", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "tokenized_reuters['training/1684']", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Let's Assume that we are interested in words containing alphabetic characters only \nand would like to remove all the other tokens\nthat contain digits, punctuation and the other symbols.\nRemoving all the non-alphabetic words from the vocabulary is\nusually required in some text analysis tasks, such as Topic Modelling that\nlearns the semantic meaning of documents.\nIt can be easily done with the  <font color=\"blue\">isalpha()</font> function.\n <font color=\"blue\">isalpha()</font>\nchecks whether the string consists of alphabetic characters only or not.\nThis method returns true if all characters in the string are in the alphabet and there \nis at least one character, false otherwise.\nIf you would like to keep all words with alphanumeric characters, you can use\n <font color=\"blue\">isalnum()</font>. Refer to Python's [built-in types](https://docs.python.org/2/library/stdtypes.html) for more detail.\nIndeed, you can construct your tokenizer in a way such that the tokenizer only extracts words with either \nalphabetic or alphanumerical characters, as we discussed in the previous chapter.\nWe will leave this as a simple exercise for you to do on your own.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "for k, v in tokenized_reuters.items():\n    tokenized_reuters[k] = [word for word in v if word.isalpha()]\n\ntokenized_reuters['training/1684']", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now you should have derived much cleaner text for each Reuters article.\nLet's check how many types we have in the whole corpus and the lexical diversity (i.e., the average number \nof times a type apprearing in the collection.)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from __future__ import division\nfrom itertools import chain\n\nwords = list(chain.from_iterable(tokenized_reuters.values()))\nvocab = set(words)\nlexical_diversity = len(words)/len(vocab)\nprint (\"Vocabulary size: \",len(vocab),\"\\nTotal number of tokens: \", len(words), \\\n\"\\nLexical diversity: \", lexical_diversity)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "There are about 1.27 million word tokens in the tokenized Reuters corpus.\nThe vocabulary size is 27,944, which is still quite large according to our knowledge of this corpus.\nThe lexical diversity tells us that words occur on average about 46 times each.\nYou might think that\nthere could still be words that occur very frequently, such as stopwords,\nand those that only occur once or twice.\nFor example, if an article \"the\" appears in almost\nevery document in a corpus,\nit might not help you at all and would only contribute noise.\nSimilarly if a word appears only once in a corpus or only in one document of the corpus,\nit could carry little useful information for downstream analysis.\nTherefore, we would better remove those words from the vocabulary, which\nwill benefit the text analysis algorithms in terms of reducing running time and\nmemory requirement, and improving their performance.\nTo do so, we need to further explore the corpus by computing some simple\nstatistics.\n\nNote that we introduced two new Python libraries in the code above.\nThey are\n[`__future__`](https://docs.python.org/2/library/__future__.html) \nand [`itertools`](https://docs.python.org/2/library/itertools.html). \nThe first statement in the code makes sure that Python switches to \nalways yielding a real result.\nThus if you divide two integer values, you will not get for example. \n````\n    1/2 = 0\n    3/2 = 1\n````\nInstead, you will have\n```\n    1/2 = 0.5\n    3/2 = 1.5\n```\nThe second statement imported a  <font color=\"blue\">chain()</font> iterator from the  <font color=\"blue\">itertools</font> module.\nWe use the iterator to join all the words in all the Reuters articles together.\nIt works as\n```python\n   for wordList in tokenized_reuters.values():\n       for word in wordList:\n           yield word\n```", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 1.2. Removing the Most and Less Frequent Words\nIt is quite useful for us to identify the words that are most informative about the sematic \nmeaning of the text regardless of syntax.\nOne common statistics often used in text processing is frequency distribution.\nIt can tell us how frequent a word is in a given corpus in terms of either term frequency or document frequency.\nTerm frequency counts the number of times a word occurs in the whole corpus regardless which document it is in.\nFrequency distribution based on term frequency tells us how the total number of word tokens are distributed across all the types.\nNLTK provides a built-in function `FreqDist` to compute this distribution directly from a set of word tokens.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from nltk.probability import *\nfd_1 = FreqDist(words)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "What are the most frequent words in the corpus?\nwe can use the  <font color=\"blue\">most_common</font> function to print out the most frequent words together with their frequencies.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "fd_1.most_common(25)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "The list above contains the 25 most frequent words.\nYou can see that it is mostly dominated by the little words of the English language which have important grammatical roles.\nThose words are articles, prepositions, pronouns, auxiliary webs, conjunctions, etc.\nThey are usually referred to as function words in linguistics, which tell us nothing about \nthe meaning of the text.\nWhat proportion of the text is taken up with such words?\nWe can generate a cumulative frequency plot for them\nusing  <font color=\"blue\">fd.plot(25, cumulative=True)</font>.\nIf you set  <font color=\"blue\">cumulative</font> to  <font color=\"blue\">False</font>, \nit will plot the frequencies of these 25 words.\nThese 25 words account for about 33% of the while Reuters corpus.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "fd_1.plot(25, cumulative=True)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "What are the most frequent words in terms of document frequency?\nHere we are going to count how many documents a word appears in, which is referred to as document frequency.\nInstead of writing nested FOR loops to count the document frequency for each word,\nwe can use  <font color=\"blue\">FreqDist()</font> jointly with  <font color=\"blue\">set()</font> as follows:\n1. Apply  <font color=\"blue\">set()</font> to each Reuters article to generate a set of unique words in the article and save all sets in a list\n```python\n    [set(value) for value in tokenized_reuters.values()]\n```\n2. Similar to what we have done before, we put all the words in a list using  <font color=\"blue\">chain.from_iterable</font> and past\nit to  <font color=\"blue\">FreqDist</font>.\n\nThe first step makes sure that each word in an article appears only once, thus the total number of \ntimes a word appears in all the sets is equal to the number of documents containing that word.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "words_2 = list(chain.from_iterable([set(value) for value in tokenized_reuters.values()]))\nfd_2 = FreqDist(words_2)\nfd_2.most_common(25)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "What you will find is that the majority of the most frequent words according to their document frequecy are still functional words.\nTherefore, the next step is to remove all the stopwords.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### 1.2.1 Ignoring Stopwords\n\nWe often remove function words from the text completely for most text analysis tasks.\nInstead of using the built-in stopword list of NLTK, we use a much rich stopword list.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "!pip install wget", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "import wget\n\nlink_to_data = 'https://github.com/tulip-lab/sit742/raw/master/Jupyter/data/stopwords_en.txt'\n\nDataSet = wget.download(link_to_data)\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "!ls", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "stopwords = []\nwith open('stopwords_en.txt') as f:\n    stopwords = f.read().splitlines()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "tokenized_reuters_1 = {}\nfor fileid in reuters.fileids():\n    tokenized_reuters_1[fileid] = [w for w in tokenized_reuters[fileid] if w not in stopwords]", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "The list comprehension \n```python\n    [w for w in tokenized_reuters[fileid] if w not in stopwords]\n```\nsays: For each word in each Reuters article, keep the word if the word is not contained in the stopword list.\n\nChecking for membership of a value in a list takes time proportional to the list's length in the average and worst cases. \nIt causes the above code to run quite slow as we need to do the check for every word in each Reuters article\nand the size of the stopword list is large.\nHowever, if you have hashable items, which means both the item order and duplicates are disregarded, \nPython `set` is better choice than `list`. The former runs much faster than the latter in terms of searching\na large number of hashable items. Indeed, `set` takes constant time to check the membership.\nLet's try converting the stopword list into a stopword set, then search to remove all the stopwords.\nPlease also note that if you try to perform iteration, `list` is much better than `set`.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "stopwordsSet = set(stopwords)\nfor fileid in reuters.fileids():\n    tokenized_reuters[fileid] = [w for w in tokenized_reuters[fileid] if w not in stopwordsSet]", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "In the above stopping process, 481 stopwords have been removed from the vocabulary. You might wonder what those removed words are. It is quite easy to check those words by differentiating the vocabulary before and after stopping. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "words_3 = list(chain.from_iterable(tokenized_reuters.values()))\nfd_3 = FreqDist(words_3)\nlist(vocab - set(fd_3.keys()))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Beside stopwords, there might some other words that occur quite often as well.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "fd_3.most_common(10)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Before we decide to remove those words from our vocabulary, it might be worth checking what \nthose words mean and the context of those words. Fortunately NLTK provides a `concordance`\nfunction in the `nltk.text` module. A concordance view shows us every occurrence of a given \nword, together with the corresponding context. For example,", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "nltk.Text(reuters.words()).concordance('mln')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "nltk.Text(reuters.words()).concordance('net')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "After reviewing those words, you might also want to remove them from the vocabulary. \nWe will leave it as an excersie for you to do on your own.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### 1.2.2 Remove Less Frequent Words\n\nIf the most common words do not benefit the downstream text analysis tasks, except for contributing noises,\nhow about the words that occur once or twice?\nHere another interesting statistic to look at is the frequency of the frequencies of word types in a given corpus.\nWe would like to see how many words appear only once, how many words appear twice, how many\nwords appear three times, and so on.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "ffd = FreqDist(fd_3.values())\nfrom pylab import *\ny = [0]*14\nfor k, v in ffd.items():\n     if k <= 10:\n        y[k-1] = v\n     elif k >10 and k <= 50:\n        y[10] =  y[10] + v\n     elif k >50 and k <= 100:\n        y[11] =  y[11] + v\n     elif k > 100 and k <= 500:\n        y[12] =  y[12] + v\n     else:\n        y[13] =  y[13] + v\nx = range(1, 15) # generate integer from 1 to 14\nytks =list(map(str, range(1, 11))) # covert a integer list to a string list\nytks.append('10-50')\nytks.append('51-100')\nytks.append('101-500')\nytks.append('>500')\nbarh(x,y, align='center')\nyticks(x, ytks)\nxlabel('Frequency of Frequency')\nylabel('Word Frequency')\ngrid(True)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "The horizontal bar chart generated above shows how many word types occur with a certain frequency.\nThere are 241 types occurring over 500 times and therefore individually accounting for about 1% of\nthe vocabulary. \nHowever, on the other extreme, more than one-third of the word types occur only once in the Reuters corpus.\nNote that the majority of word types occur quite infrequently given the size of the whole corpus (i.e., 721,371 word tokens):\nabout 78% of the word types occur 10 times or less.\nSimilarly, you can also look at the bar chart based on the document frequency. Try it by yourself!\n\nLet's further remove those words that occur only once. \nTo get those words, you can write the code like\n```python\n    lessFreqWords = set([k for k, v in fdist.items() if v < 2])\n```\nor choose to use `hapaxes()` function.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "lessFreqWords = set(fd_3.hapaxes())\n\ndef removeLessFreqWords(fileid):\n    return (fileid, [w for w in tokenized_reuters[fileid] if w not in lessFreqWords])\n\n#pool = mp.Pool(4)\n#tokenized_reuters = dict(pool.map(removeLessFreqWords, reuters.fileids()))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "tokenized_reuters = dict(removeLessFreqWords(fileid) for fileid in reuters.fileids())", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now, you should have a pretty clean set of Reuters articles, each of which is stored as a list of word tokens.\nLet's further print out some statistics that summarize this corpus.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import numpy as np\nwords = list(chain.from_iterable(tokenized_reuters.values()))\nvocab = set(words)\nprint (\"Vocabulary size: \",len(vocab))\nprint (\"Total number of tokens: \", len(words))\nprint (\"Lexical diversity: \", lexical_diversity)\nprint (\"Total number of articles:\", len(tokenized_reuters))\nlens = [len(value) for value in tokenized_reuters.values()]\nprint (\"Average document length:\", np.mean(lens))\nprint (\"Maximun document length:\", np.max(lens))\nprint (\"Minimun document length:\", np.min(lens))\nprint (\"Standard deviation of document length:\", np.std(lens))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "It is interesting that the minimun document length is 0. There must be some Reuters articles that are extremely short,\nafter tokenization and stopping, there are no words left. Can you check those documents to see what they look like?", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Part 2. Building Vector Representation\n\nAfter text pre-processing has been completed, each individual document needs to be transformed into \nsome kind of numeric representation that can be input into most NLP and text mining algorithms.\nFor example, classification algorithms, such as Support Vector Machine, can only take data in a \nstructured and numerical form. They do not accept free languge text.\nThe most popular structured representation of text is the vector-space model, which represents text\nas a vector where the elements of the vector indicate the occurence of words within the text.\nThe vector-space model makes an implicit assumption that \nthe order of words in a text document are not as\nimportant as words themselves, and thus disregarded.\nThis assumpiton is called [**Bag-of-words**](https://en.wikipedia.org/wiki/Bag-of-words_model).\n\nGiven a set of documents and a pre-defined list of words appearing \nin those documents (i.e., a vocabulary), you can compute a vector representation for each document.\nThis vector representation can take one of the following three forms:\n* a binary representation,\n* an integer count,\n* and a float-valued weighted vector.\n\nTo highlight the difference among the three approaches, we use a very simple example as follows:\n```\n    document_1: \"Data analysis is important.\"\n    document_2: \"Data wrangling is as important as data analysis.\"\n    document_3: \"Data science contains data analysis and data wrangling.\"\n```\nThe three documents contain 20 tokens and 9 unique words.\nThose unique words are sorted alphabetically with total counts:\n```\n     'analysis': 3,\n     'and': 1,\n     'as': 2,\n     'contains': 1,\n     'data': 6,\n     'important': 2,\n     'is': 2,\n     'science': 1,\n     'wrangling': 2\n```\nGiven the vocabulary above, \nboth the binary and the integer count vectors are easy to compute.\nA binary vector stores 1s for the word that appears in a document and 0s for the other words in\nthe vocabulary,\nwhereas a count vector stores the frequency of each word appearing in the document.\nThus, the binary vector representations for the three documents above are\n   \n   ||'analysis'|'and'|'as'|'contains'|'data'|'important'|'is'|'science'|'wrangling'|\n   |-|-|-|-|-|-|-|-|-|\n   |document 1:|1|0|0|0|1|1|1|0|0|\n   |document 2:|1|0|1|0|1|1|1|0|1|\n   |document 3:|1|1|0|1|1|0|0|1|1|\n\nThe count vector representations for the same documents would look as follows:\n\n   ||'analysis'|'and'|'as'|'contains'|'data'|'important'|'is'|'science'|'wrangling'|\n   |-|-|-|-|-|-|-|-|-|\n   |document 1:|1|0|0|0|1|1|1|0|0|\n   |document 2:|1|0|2|0|2|1|1|0|1|\n   |document 3:|1|1|0|1|3|0|0|1|1|\n\nInstead of using the two vector representations above, \nmost existing text analysis algorithms, like document classification and information retrieval, \nprefer representing documents as weighted vectors.\nThe raw term frequency is often replaced with a weighted term frequency\nthat indicates how important a word is in a particular document.\nThere are many different term weighting schemes online.\nTo store each document as a weighted vector, we first need to choose a weighting scheme. \nThe most popular scheme is the TF-IDF weighting approach. \nTF-IDF stands for term frequency-inverse document frequency. \nThe term frequency for a word is the number of times the word appears in a document. \nIn the preceding example, the term frequency in Document 2 for \u201cdata\u201d is 2, since it appears twice in the document. Document frequency for a word is the number of documents that contain the word; \nit would also be 3 for \u201cdata\u201d in the collection of the three preceding documents. \nThe Wikipidia entry on [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) lists \na number of variants of TF-IDF. \nOne variant is reproduced here\n$$tf\\cdot idf(w,d) = tf(w, d) * idf(w)$$\n\nwhere \n\n$$tf(w,d)\\,=\\, \\sum_{i}^{|d|} 1_{w = w_{d,i}}$$\nand\n$$idf(w) = log\\left(\\frac{|D|}{|d \\in D: w \\in d |}\\right)$$\n\nThe assumption behind TF-IDF is that words with high term frequency should receive high weight unless they also have high document frequency. \nStopwords are the most commonly occurring words in the English language. They often occur many times within a single document, but they also occur in nearly every document. \nThese two competing effects cancel out to give them low weights,\nas those very common words carry very little meaningful information about the actual contents of the document.\nTherefore, the TF-IDF weights for stopwords are almost always 0.\nWith the TF-DF formulas above,\nthe weighted vector representations for the example documents are computed as\n\n||'analysis'|'and'|'as'|'contains'|'data'|'important'|'is'|'science'|'wrangling'|\n   |-|-|-|-|-|-|-|-|-|\n   |document 1:|0|0|0|0|0|0.176|0.176|0|0|\n   |document 2:|0|0|0.954|0|0|0.176|0.176|0|0.176|\n   |document 3:|0|0.477|0|0.477|0|0|0|0.477|0.176|", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Given the cleaned up Reuters documents, how can we generate those vectors for each documents? \nUnfortunately, NLTK does not implement methods that directly produce those vectors.\nTherefore, we will either write our own code to compute them or appeal to other data analysis libraries.\nHere we are going to use [scikit-learn](http://scikit-learn.org/stable/index.html), an open source machine \nlearning library for Python.\nIf you use Anaconda, you should already have scikit-learn installed, otherwise you will need to \n[install it](http://scikit-learn.org/stable/install.html) by following the instruction on its official website.\n\nAlthough scikit-learn features various classification, regression and clustering algorithms\nwe are particularly interested in its feature extraction module, [sklearn.feature_extraction](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction).\nThis module is often used to \"extract features in a format supported by machine learning algorithms from datasets consisting of formats such as text and image.\" Please refer to its documentation on text feature extraction,\nsection 4.2.3 of [Feature Extraction](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction). We will demonstrate the usage of the following two classes:\n* [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer): It converts a collection of text documents to a matrix of token counts. \n* [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer):\nIt converts a collection of raw documents to a matrix of TF-IDF features.\n\n### 2.1 Creating Count Vectors\nLet's start with generating the count vector representation for each Reuters document.\nInitialise the \"CountVector\" object: since we have pre-processed all the Reuters documents, \nthe parameters, \"tokenizer\", \"preprocessor\" and \"stop_words\" are set to their default value, i.e., None.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(analyzer = \"word\") ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Next, transform Reuters articles into feature vectors. `fit_transform` does two things: First, it fits the model and learns the vocabulary; second it transforms the text data into feature vectors. \nPlease note the input to `fit_transform` should be a list of strings. \nSince we have stored each tokenised article as a list of words, we concatenate all the words in the list and separate\nthem with white spaces. \nThe following code will do that:\n```python\n[' '.join(value) for value in tokenized_reuters.values()]\n```\nThen, we input this list of strings into `fit_transform`,", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "data_features = vectorizer.fit_transform([' '.join(value) for value in tokenized_reuters.values()])\nprint (data_features.shape)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "The shape of document-by-word matrix should be 10788 * 17403. \nHowever, in order to save such a matrix in memory but also to speed up algebraic operations on the matrix,\nscikit-learn implements matrix/vector in a sparse representation.\nLet's check the count vector for the first article, i.e., 'training/1684'.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "vocab2 = vectorizer.get_feature_names()\nfor word, count in zip(vocab, data_features.toarray()[0]):\n    if count > 0:\n        print (word, \":\", count)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Another way to get the count list above is to use `FreqDist`.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "FreqDist(tokenized_reuters['training/1684'])", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Note that the vocabulary you just got with `vectorizer.get_feature_names()`  shoud be exactly the same\nas the one you got in section 1. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "list(vocab-set(vocab2))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "### 2.2 Creating TF-IDF Vectors\nSimilar to the use of `CountVector`, we first initialise a `TfidfVectorizer` object by only specifying \nthe value of \"analyzer\", and then covert the Reuters data into a list of strings, each of which corresponds\nto a Reuters articles.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(analyzer = \"word\")\ntfs = tfidf.fit_transform([' '.join(value) for value in tokenized_reuters.values()])\ntfs.shape", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Let's print out the weighted vector for the first document.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "vocab = vectorizer.get_feature_names()\nfor word, weight in zip(vocab, tfs.toarray()[0]):\n    if weight > 0:\n        print (word, \":\", weight)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "So now we have converted all the Reuters articles into feature vectors. \nWe can use those vectors to, for example,\n* compute the similarity between two articles, \n* search articles for a given query\n* do other advance text analysis, such as document classification and clustering.\n\nAssume that we have a new document, how can we get its TF-IDF vector.\nWe do this by using the transform function as follows.\nWe have randomly chosen a sentence from \n[a recent Reuters news](http://www.reuters.com/article/us-usa-election-idUSKCN0W346T).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "str = \"\"\"\nthe former secretary of state hoped to win enough states to take a big step toward wrapping up her nomination fight\nwith a democratic senator from Vermont.\n\"\"\"\nresponse = tfidf.transform([str])\nfor col in response.nonzero()[1]:\n    print (vocab[col], ' - ', response[0, col])", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Note that the text above is not included in the trained TF-IDF model with the 'transform' function, unless the `fit_transform` function is called,\n\nBoth `CountVectorizer` and `TfidfVectorizer` come with their own options to automatically do pre-processing, tokenization, and stop word removal -- for each of these, instead of using their default value (i.e., None),\nwe could customise the two vectorizer classes by either using a built-in method or specifying our own function.\nSee the function documentation for more details.\nHowever, we wanted to write our own function for clean the text data in this chapter to show you how \nit's done step by step.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Part 3. Saving Pre-processed Text to a File\n\nThe pre-processed text needs to be saved in a proper format so that it can be easily used by the downstream analysis algorithm. There are a couple of ways of dumping the pre-processed text data into txt files. \nFor example, use one txt file to store the tokenized documents. The tokens in a document are stored in one row in the txt file, and are separated with a given delimiter, e.g., whitespace. In this case, the downstream text analyser needs to re-construct the vocabulary.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import wget\n\nlink_to_data = 'https://github.com/tulip-lab/sit742/raw/master/Jupyter/data/reuters_1.txt'\n\nDataSet = wget.download(link_to_data)\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# import wget\n\nlink_to_data = 'https://github.com/tulip-lab/sit742/raw/master/Jupyter/data/reuters_2.txt'\n\nDataSet = wget.download(link_to_data)\n\n!ls", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "out_file = open(\"reuters_1.txt\", 'w')\nfor d in tokenized_reuters.values():\n    out_file.write(' '.join(d) + '\\n')\nout_file.close()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "You can also save vocabulary in a separate file, and assign a fixed integer id to each word in the vocabulary. What text analysers usually do is to use the index of each word in the vocabulary as its integer id.\nGiven the vocabulary, each document can be represented as a sequence of integers that correspond to the tokens,\nor in the following sparse form:\n```\n    word_index:word count\n```\nfor example,", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "out_file = open(\"reuters_2.txt\", 'w')\nvocab = list(vocab)\n\nvocab_dict = {}\ni = 0\nfor w in vocab:\n    vocab_dict[w] = i\n    i = i + 1\n\nfor d in tokenized_reuters.values():\n    d_idx = [vocab_dict[w] for w in d]\n    for k, v in FreqDist(d_idx).items():\n        out_file.write(\"{}:{} \".format(k,v))\n    out_file.write('\\n')\nout_file.close()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## Part 4. Extracting Other Features\n\nIt is common for most text analysis tasks to treat documents as bags-of-words, which can significantly simplify the inference procedure of text analysis algorithms. \nHowever, things always have pros and cons. \nThe bag-of-words representation loses lots of information encoded in either syntax or word order (i.e., dependencies between adjacent words in sentences.). \nFor example, representing a document as a collection of unigrams effectively disregards any word order dependence,\nwhich fails to capture phrases and multi-word expressions. A similar issue has been mentioned in section 2.1. of Chapter 2. \nIn this section, we are going to show you how to\n* use Part-of-Speeching (POS) tagging to extract specific word groups, such as all nouns, verbs, etc.,\n* extract n-grams,\n*  and extract collocations\n\nThese features can be further used to enrich the representation of a document.\n\n### 4.1 Extracting Nouns and Verbs\n\nIt is easy for human to tell the difference between nouns, verbs, \nadjectives and adverbs, as we have learnt them back in elementary school.\nHowever, how can we automatically classify words into their parts of speech (i.e., lexical categories or word classes) \nand label them accordingly with computer program? \nThis section is not going to discuss how to determine the category of a word from a linguistic perspective.\nInstead it demonstrates the use of some existing POS taggers to extract words in a specific lexical category.\nIt has been proven that words together with their part-of-speech (POS) are quite useful for many language processing tasks. \n\nIn NLP, the process of labelling words with their corresponding part-of-speech (POS) tags is known as [POS tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging).\nA POS tagger processes a sequence of words and attaches a POS tag to each word based on both its definition and its context. There are many POS taggers available online, such as [Sandford POS tagger](http://nlp.stanford.edu/software/tagger.shtml). \nWe are going to use the one implemented by NLTK.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "example_sent = 'A POS tagger processes a sequence of words and attaches a POS tag to each \\\nword based on both its definition and its context'\ntext = nltk.word_tokenize(example_sent)\ntagged_sent = nltk.tag.pos_tag(text)\nprint (tagged_sent)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "If you are seeing these tags for the first time, you will wonder what these tags mean. \nYou can find the specification of all the tags [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html). \nNLTK provides documentation for each tag, which can be queried using the tag, e.g., ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "nltk.download('tagsets')\nprint (nltk.help.upenn_tagset('NNP'))\nprint (nltk.help.upenn_tagset('IN'))\nprint (nltk.help.upenn_tagset('PRP$'))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "The example sentence has been processed by `pos_tag` into a list of tuples, each of which is a pair of a word and its POS tag. We see that 'a' is 'DT', a determiner; 'its' is 'PRP$', a possessive pronoun; 'and' is 'CC', a coordinating conjunction, 'words' is 'NNS', a noun in the plural form, and so on. Note that several of the corpora included in NLTK have been tagged for their POS. Please click [here](http://www.nltk.org/howto/corpus.html#tagged-corpora) to see how to access those tagged corpora.\nHere is an example of using the `tagged_words` function to retrieve all words in Brown corpus with their tags.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "nltk.download('brown')\nnltk.corpus.brown.tagged_words()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Please note that the collection of tags is known as a tag set. \nThere are many different conventions for tagging words.\nTherefore, tag sets can vary among different tasks.\nWhat we used above is the Penn Treebank tag set.\nLet's change the tag set to the Universal POS tag set, and print the Brown corpus again.\nYou will find different tags are used.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "nltk.download('universal_tagset')\nnltk.corpus.brown.tagged_words(tagset='universal')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "If you would like to learn more about POS tagging, please refer to [1].\n\nGiven the tagged text, you can easily identify all the nouns, verbs, etc.\nNouns generally refer to people, places, things, or concepts, e.g., Monash, Melbourne, university, data, and science. \nNouns can appear after determiners and adjectives, and can be the subject or object of the verb.\nNow how can we extract all the nouns from a text?\nAssume we use the Penn Treebank tag set.\nHere are all the tags for nouns:\n```\n    NN    Noun, singular or mass\n    NNS   Noun, plural\n    NNP   Proper noun, singular\n    NNPS  Proper noun, plural\n```\nIt is not hard to see all the tags above start with 'NN'.\nThus, we can iterate over all the words and check if their tag string starts with 'NN'.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "all_nouns = [w for w,t in tagged_sent if t.startswith('NN')]\nall_nouns", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Similarly, you will find that all the verb tags start with 'VB', see\n```\n    VB\tVerb, base form\n    VBD   Verb, past tense\n    VBG   Verb, gerund or present participle\n    VBN   Verb, past participle\n    VBP   Verb, non-3rd person singular present\n    VBZ   Verb, 3rd person singular present\n```\nThus,", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "all_verbs = [w for w,t in tagged_sent if t.startswith('VB')]\nall_verbs", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Unfortunately, the Reuters corpus that we have been using, has no built-in POS tags. But you can get sentences from Reuters corpus, and then you can get the POS tags.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 4.2 Extracting N-grams and Collocations\n\nBesides unigrams that we have been working on so far,\nN-grams of texts are also extensively used in various text analysis tasks.\nThey are basically contiguous sequences of `n` words from a given sequence of text.\nWhen computing the n-grams you typically move a fixed size window of size n\nwords forward.\nFor example, for the sentence\n\"Laughter is like a windshield wiper.\"\nif N = 2 (known as bigrams), the n-grams would be:\n```\n    Laughter is \n    is like \n    like a \n    a windshield \n    windshield wiper\n```\nSo you have 5 bigrams in this case. Notice that the generative process above\nessentially moves one word forward to generate the next bigram.\nIf N = 3 (known as trigrams), the n-grams would be:\n```\n    Laughter is like \n    is like a \n    like a  windshield\n    a  windshield wiper\n```\nWhat are N-grams used for? They can be used to build n-gram language model that\ncan be further used for speech recognition, spelling correction, entity detection, etc.\nIn terms of text mining tasks, n-grams is used for developing features for \nclassification algorithms, such as SVMs, MaxEnt models, Naive Bayes, etc.\nThe idea is to expand the unigram feature space with n-grams.\nBut please notice that\nthe use of bigrams and trigrams in your feature space may not necessarily yield significant performance\nimprovement. The only way to know this is to try it! \nExtracting from a text a list of n-gram can be easily accomplished with function `ngram()`:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from nltk.util import ngrams\nbigrams = ngrams(reuters.words(), n = 2)\nfdbigram = FreqDist(bigrams)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "fdbigram.most_common()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Collocations are expressions of multiple words that commonly co-occur. \n\n>Finding collocations requires first calculating the frequencies of words and\ntheir appearance in the context of other words. Often the collection of words\nwill then requiring filtering to only retain useful content terms. Each ngram\nof words may then be scored according to some association measure, in order\nto determine the relative likelihood of each ngram being a collocation. (Quoted from [here](http://www.nltk.org/_modules/nltk/collocations.html))\n\nFor example, to extract bigram collocations, we can firstly extract bigrams then get the commonly co-occurring ones by ranking the bigrams by some measures. A commonly used measure is [Pointwise Mutual Information](https://en.wikipedia.org/wiki/Pointwise_mutual_information) (PMI). The following code will find the best 50 bigrams using the PMI scores.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "bigram_measures = nltk.collocations.BigramAssocMeasures()\nfinder = nltk.collocations.BigramCollocationFinder.from_words(reuters.words())\nfinder.nbest(bigram_measures.pmi, 50)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "The `collocations` module implements a number of measures to score collocations or other associations. \nThey include Student's t test, Chi-Square, likelihood ratios, PMI and so on.\nHere we used PMI scores for finding bigrams.\nPlease read [2] for a detailed tutorial on finding collocations with NLTK.\nIf you would like to know more about collocations, please refer to [3].", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Part 5. Summary\n\nThis chapter has show you how to \n\n* generate vocabulary be further exploring the tokenized text with some simple statistics. \n* convert unstructured text to structured form using the bag-of-words model\n* compute TF-IDF\n* extract words in specific lexical categories, n-grams and collocations.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Part 6. Reading Materials\n\n1. \"[Categorizaing and Tagging Words](http://www.nltk.org/book/ch05.html)\", \nChapter 5 of \"Natural Language Processing with Python\".\n2. \"[Collocations](http://www.nltk.org/howto/collocations.html)\": An NTLK tutorial on how to extract collocations \ud83d\udcd6 .\n3. \"[Collocations](http://nlp.stanford.edu/fsnlp/promo/colloc.pdf)\": An introduction to collocation by Manning and Schutze.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Part 7 . Exercises\n\n2. We have shown you how to generate frequency of frequency bar chart with term frequency. Similarly, you can generate the bar chart based on document frequency. \n2. Remove short words. There are some very short words in the vocabulary, for example, 'aa', 'ab', 'ad', 'ax', etc.\nWrite Python code to explore the distribution of word lengths, and remove those words with less than two characters.\n3. Write code to tag the Reuters corpus with the Penn Treebank tag set, find the top 10 most common tags, nouns, and verbs.\n2. There might be some text analysis tasks where the binary occurrence markers might be enough. \nPlease modify the CountVectorizer code to generate binary vectors for all the Reuters articles. \n2. We have shown you how to generate feature vectors from raw text. As we mentioned in section 3, you can actually customise the two vectorizer classes by specifying, for example, the tokenizer and stopword list. So try\nto customize either vecotorizer so that it can carry out all the steps in section 1.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}