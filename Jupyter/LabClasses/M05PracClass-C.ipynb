{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Lab M05 (Version 3)\n",
    "## Time Series Forecasting, Anomaly and Rule mining\n",
    "\n",
    "Practice Lab M04 will focus on how to do the time series forecasting, anomaly detection and rule mining\n",
    "\n",
    "## To do\n",
    "\n",
    "- Use ARIMA to predict the time series\n",
    "- Using isolation forest to find the anomaly\n",
    "- Conduct the association rule mining.\n",
    "\n",
    "\n",
    "## Tasks 1 Time series and Anomaly detection\n",
    "### Task 1.1 Reading time series\n",
    "We first read the given time series (https://raw.githubusercontent.com/tulip-lab/open-data/master/HK2012-2018/United_States.csv or the 'US_HK.csv' in data folder). However, before doing any forecasting, we will need to define some error metrics for evaluation purpose. In there we will use mean abusolute percentage error and mean abusolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IokF39Gw9h58",
    "outputId": "2f3e6fad-eccd-4bd8-d3ce-981b41ecaa23"
   },
   "outputs": [],
   "source": [
    "!pip install apyori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wefHkmVVzbU"
   },
   "outputs": [],
   "source": [
    "import warnings                                  # `do not disturbe` mode\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np                               # vectors and matrices\n",
    "import pandas as pd                              # tables and data manipulations\n",
    "import matplotlib.pyplot as plt                  # plots\n",
    "import seaborn as sns                            # more plots\n",
    "\n",
    "from dateutil.relativedelta import relativedelta # working with dates with style\n",
    "from scipy.optimize import minimize              # for function minimization\n",
    "\n",
    "import statsmodels.formula.api as smf            # statistics and econometrics\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as scs\n",
    "\n",
    "from itertools import product                    # some useful functions\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CxAdP1bPhNe5"
   },
   "outputs": [],
   "source": [
    "#- read the time series by using pandas as 'ads' \n",
    "ads = pd.read_csv('xxx', index_col=['date'], parse_dates=['date'])\n",
    "ads.index = pd.to_datetime(ads.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "izx_Whl_mtoQ",
    "outputId": "d26e0992-762b-4c3c-9484-ee4ac7d70adb"
   },
   "outputs": [],
   "source": [
    "# plot the time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bNj6a-LnNBH"
   },
   "source": [
    "Mean Absolute Percentage Error: this is the same as MAE but is computed as a percentage, which is very convenient when you want to explain the quality of the model to management, $[0, +\\infty)$\n",
    "\n",
    "$\\texttt{MAPE} = \\frac{100}{n}\\sum\\limits_{i=1}^{n} \\frac{|y_i - \\hat{y}_i|}{y_i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UEeMRRNwm29n"
   },
   "outputs": [],
   "source": [
    "# - defne the function of mean abusolute percentage error in numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 Moving average smoothing forecasting\n",
    "After having the time series, let's do Moving Average Smoothing Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPLqRadTnPvX"
   },
   "source": [
    "Let's start with a naive hypothesis: \"tomorrow will be the same as today\". However, instead of a model like $\\hat{y}_{t} = y_{t-1}$ (which is actually a great baseline for any time series prediction problems and sometimes is impossible to beat), we will assume that the future value of our variable depends on the average of its $k$ previous values. Therefore, we will use the **moving average**.\n",
    "\n",
    "$\\hat{y}_{t} = \\frac{1}{k} \\displaystyle\\sum^{k}_{n=1} y_{t-n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RYy3E26bnXiP",
    "outputId": "a5a9e427-b62e-4662-9e2e-0a5bc7f62983"
   },
   "outputs": [],
   "source": [
    "# define the moving average function by numpy\n",
    "def moving_average(series, n):\n",
    "    \"\"\"\n",
    "        Calculate average of last n observations\n",
    "    \"\"\"\n",
    "    return xxxx\n",
    "\n",
    "moving_average(ads, 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_QNguFxnnYn"
   },
   "source": [
    "moving average has another use case - smoothing the original time series to identify trends. Pandas has an implementation available with [`DataFrame.rolling(window).mean()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rolling.html). The wider the window, the smoother the trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRDdd7NlnatO"
   },
   "outputs": [],
   "source": [
    "series = ads.arrival\n",
    "window = 3\n",
    "# use pandas to conduct the moving average with window size of 3\n",
    "rolling_mean = series.rolling(window=window).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "u5YrLnx_oCa-",
    "outputId": "93027eaf-0bdd-4921-b038-c13d966d1cff"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import  mean_absolute_error\n",
    "mae = mean_absolute_error(series[window:], rolling_mean[window:])\n",
    "deviation = np.std(series[window:] - rolling_mean[window:])\n",
    "lower_bond = rolling_mean - (mae + 1 * deviation)\n",
    "upper_bond = rolling_mean + (mae + 1 * deviation)\n",
    "# plot the upper bound and lower bound by using the mean abusolute error + 1*standard deviation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnwTq28EuT6-"
   },
   "source": [
    "### Task 1.3 Anomaly detection\n",
    "After having the original time series, the moving avergae and the upper bound and lower bound in one plot, we could see some anomalies, could we use isolation to detect them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XV2aEzUquiTv",
    "outputId": "4faef8d9-2166-4bb4-d855-16943b827aaf"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "# write code for training the isolation on original time series\n",
    "rng = np.random.RandomState(0)\n",
    "clf = IsolationForest(max_samples=100, random_state=rng)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the anomalies in original time series and mark them in the plot with original time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eovxR9BgxG1w"
   },
   "source": [
    "### Task 1.4 Exponential smoothing forecasting\n",
    "Now we would like to try the exponential smoothing forecasting.\n",
    "\n",
    "Now, let's see what happens if, instead of weighting the last $k$ values of the time series, we start weighting all available observations while exponentially decreasing the weights as we move further back in time. There exists a formula for **[exponential smoothing](https://en.wikipedia.org/wiki/Exponential_smoothing)** that will help us with this:\n",
    "\n",
    "$$\\hat{y}_{t} = \\alpha \\cdot y_t + (1-\\alpha) \\cdot \\hat y_{t-1} $$\n",
    "\n",
    "Here the model value is a weighted average between the current true value and the previous model values. The $\\alpha$ weight is called a smoothing factor. It defines how quickly we will \"forget\" the last available true observation. The smaller $\\alpha$ is, the more influence the previous observations have and the smoother the series is.\n",
    "\n",
    "Exponentiality is hidden in the recursiveness of the function -- we multiply by $(1-\\alpha)$ each time, which already contains a multiplication by $(1-\\alpha)$ of previous model values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xRIBqwQ2yOl0"
   },
   "outputs": [],
   "source": [
    "def exponential_smoothing(series, alpha):\n",
    "    \"\"\"\n",
    "        series - dataset with timestamps\n",
    "        alpha - float [0.0, 1.0], smoothing parameter\n",
    "    \"\"\"\n",
    "    result = [series[0]] # first value is same as series\n",
    "    for n in range(1, len(series)):\n",
    "        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n",
    "    result = pd.DataFrame(result, index=series.index)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "tN3UPuxyyYnN",
    "outputId": "f69033b3-1035-493b-cea2-58c0594a55f7"
   },
   "outputs": [],
   "source": [
    "#Using given exponential smoothing forecasting function to obtain the new time series\n",
    "#plot the exponential smoothing forecasting time series with original together in one plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 ARIMA\n",
    "### Task 2.1 Find parameter p, d and q\n",
    "Now we want to find the parameter of p, d and q for ARIMA before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ysTnE9DtzUvU",
    "outputId": "ff41ea4f-ae2c-4df0-a69b-ef09b40d2303"
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "#let's pass the time series into the adfuller test function, firstly let's test the original\n",
    "# Use adfuller test to determine the d\n",
    "\n",
    "# let's check the test results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "VEPqDTa33zLk",
    "outputId": "3362a089-3d7b-4a3f-85a1-8cc8e0229254"
   },
   "outputs": [],
   "source": [
    "#Use pacf and acf plot to draw the auto correlation plot for p and q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdMfYPjI42CO"
   },
   "source": [
    "- $p$ is most probably x since it is the last significant lag on the PACF, after which, most others are not significant. \n",
    "- $d$ equals x because the sereis after first differential process  is quite stationary\n",
    "- $q$ should be somewhere around x as well as seen on the ACF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 Train the ARIMA\n",
    "Now we will use the parameter p, d and q to train the ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lIAzLize41a0",
    "outputId": "7578b2a4-629b-4ae6-cf68-efcc351f0fb1"
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "# train the ARIMA and plot the fitted time series with original time series together\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCjdjUAd9WgD"
   },
   "source": [
    "### Task 2.3 (Advanced) Assocaition Rule Mining\n",
    "We want to do the association rule mining, but before doing it, we will need to discrete the value from given time series\n",
    "Selecting the 3 feature from DataFrame and use Apriori to mine some rules out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fEKIjwBAh3s"
   },
   "outputs": [],
   "source": [
    "tran = ads[['arrival','Hong kong','Hong kong dollar']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-CczqmHA55S"
   },
   "outputs": [],
   "source": [
    "tran['date'] = tran.index.month_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VVJ4v4xBkLj"
   },
   "source": [
    "Discrete the 3 features into 4 equal-sized bins by using [`pd.qcut(series, 4)`](https://pandas.pydata.org/docs/reference/api/pandas.qcut.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7-T5p2MA_H0",
    "outputId": "8f8b5c5e-65ed-4dd1-e95c-5c25ad5b91b8"
   },
   "outputs": [],
   "source": [
    "newcols=[]\n",
    "for i in tran.columns[:-1]:\n",
    "  newcol = i+'_bin'\n",
    "  tran[newcol] = pd.qcut(tran[i], 4,labels=[\"low\", \"medium\", \"high\",\"high+\"])\n",
    "  print('finish_'+i)\n",
    "  newcols.append(newcol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWyVKr-4CmyS"
   },
   "outputs": [],
   "source": [
    "tran = tran[newcols+['date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "XsiKxxVyDKx6",
    "outputId": "d5ac5bd1-c72b-4811-a450-f144282c4e72"
   },
   "outputs": [],
   "source": [
    "tran.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the rule mining and print the rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KMMSQgaNERwE",
    "outputId": "1b1d10e3-c48c-4df4-c124-db5c936959f6"
   },
   "outputs": [],
   "source": [
    "for i in range(len(results)):\n",
    "    print(\"##############################################################################\")\n",
    "    print(i)\n",
    "    print(results[i])\n",
    "    print(results[i].items)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "M05PracClass-C.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
