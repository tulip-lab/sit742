{
    "nbformat_minor": 2, 
    "cells": [
        {
            "source": "# SIT742: Modern Data Science \n**(Week 11: Data Analytics (IV))**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are free to use, change and distribute this package.\n- If you found any issue/bug for this document, please submit an issue at [tulip-lab/sit742](https://github.com/tulip-lab/sit742/issues)\n\nPrepared by **SIT742 Teaching Team**\n\n---\n\n\n## Session 11B - Designing a Machine Learning System\n\n\n---\n\n**The purpose of this session is to demonstrate:**\n\n1. A typical sequence of steps in designing a machine learning algorithm\n2. How to implement these steps using open scikit-learn Python package\n\n** References and additional reading and resources**\n- [An introduction to machine learning with scikit-learn](http://scikit-learn.org/stable/tutorial/basic/tutorial.html#machine-learning-the-problem-setting)\n\n---", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": " ## <span style=\"color:#0b486b\">1. Steps to a Build Machine Learning Model</span>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "All machine learning applications start with data. As a data scientist, a major part of your daily work is to look at the data and draw insights from them. Machine learning algorithms are the underlying tools and methods for you to draw such insights. In fact, beyond just being tools, *they provide us a framework to think, generate new ideas and new data products*. As we progress along this course, I hope you will start to build up such intuition.\n\nOne of the most important forms of these \"insights\" is the prediction power from the data. The rest of this tutorial will step you through a typical setting of building such a ML-based prediction model. \n\nOur problem is **face recognition**. Our problem can simply be reduced to two sequential steps:\n1. We are provided with a collection of images, containing **40** different individuals. There are **10**  images for each individual. From this dataset, build a predictive model.\n2. When a **new (unseen)** image is presented to the system, **recognize** the individual in this image.\n\nFace-based identification systems are now being used around the world, most popularly at airports. When applying for a passport, photo of the applicant is taken. We can relate to this as step (1) where the system collects a database of face images to build the prediction model; and the authentication step at the airport is step (2). ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<img src=\"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/m01gf1.jpg\" width=\"400\">\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "***However, a fixed and predefined dataset is all a data scientist has in advance, so how can we evaluate if our predictive model will perform well after it has been deployed, since we don't have access to unseen images/photos that the system will capture in the future? ***", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "This scenario is encountered in most of the machine learning problems and this is one of the reasons that make modern data scientists different from traditional statistician's jobs.\n\nTo address this, a ML practitioner  will typically (randomly) split the given dataset $D$ into, what's called, the **training** dataset and **testing** dataset. We then use the training dataset to build the model, and then use the testing dataset, **as if the system has never seen it before**, to test how well the system will perform.\n\nThis line of logics naturally tells us three essential steps to build a machine learning pipeline:\n\n* **Step 1**. **Preparing your dataset in the form that is applicable to apply a machine leaning model** such as logistic regression, decision tree classifier and so forth. This usually also means that one might need to do necessary pre-processing steps (e.g., data cleaning) and then split the data into *training* and *testing* datasets.\n* **Step 2**. **Select a suitable or a set of suitable machine learning models** for your problem at hand and train them using the **training dataset**. This step requires a deep understanding of machine learning models available to you, when it works and it doesn't work, what input is expected, what are its pitfalls, how to interpret its outputs and so on.\n* **Step 3**. **Evaluate the performance of model** using the **testing dataset**.\n\nThis only presents the most essential steps. In practice, we usually repeat these steps several times and have several additional substeps such as: formulating sensible features (**feature extraction**), selecting important features (**feature selection**), selecting optimal model or parameters (**model selection**).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "---\n### <span style=\"color:#0b486b\">Step 1. Preparing your dataset: pre-processing, training and testing datasets</span>\nTo build a predictive model, a typical setting is to split the dataset into training and testing dataset.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "!pip install -U scikit-learn", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "**<span style=\"color:blue\"> 1.1 First, we load the dataset using scikit-learn </span>**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "%matplotlib inline\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import metrics\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# When these datasets are loaded, they aren't loaded as NumPy arrays. They are of type Bunch. \n# A Bunch is a common data structure in Python. It's essentially a dictionary with the keys added to the object as attributes.\n\nfaces = datasets.fetch_olivetti_faces()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print(faces.DESCR)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print(faces.keys())", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# this function is a utility to face images from the dataset\ndef display_faces(images, label, num2display):\n    fig = plt.figure(figsize=(25,25))\n    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n    for i in range(num2display):\n        p = fig.add_subplot(20,20,i+1,xticks=[],yticks=[])\n        p.imshow(images[i], cmap=plt.cm.bone)\n        \n        #p.text(0, 14, str(label[i]), color='red', fontsize=18)\n        #p.text(0, 60, str(i))\n    fig.show()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# display the first 20 images\ndisplay_faces(faces.images[7:20], faces.target[7:20], 6)\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# store features in variable X and the label in variable y as in our usual convention notation.\nX, y = faces.data, faces.target", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# number of class labels\nn_classes = len(np.unique(y))\nprint(n_classes)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "**<span style=\"color:blue\"> 1.2 Next, we perform a pre-processing step to transform feature.</span>** In this case we use **principal component analysis (PCA)** to perform a **dimenentionality reduction** step, which is extremely useful when dealing with high-dimentional data as typically encountered in modern data science problem.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<img src=\"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/note.gif\" width=\"50\", align=\"left\"></img> *If you are unfamiliar with PCA, it is highly recommended that you should learn and know how to apply it to your tasks. PCA involves *singular value decomposition* technique from linear algebra, which might appears somewhat complicated. From a practical point of view, it is sufficient to know that it can be used to reduce continuous-valued feature vectors in high-dimensional space to low-dimensional spaces. You can find an explanation for PCA in almost any data analysis, machine learning or AI text books. Its wikipedia entry can be found [here](https://en.wikipedia.org/wiki/Principal_component_analysis).**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# this steps further pre-process the features by performing a dimentionality reduction step via PCA.\nfrom sklearn import decomposition\npca = decomposition.PCA(n_components=10)\nprint(pca)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pca_X = pca.fit_transform(X)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pca.explained_variance_ratio_", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "**<span style=\"color:blue\"> 1.3 Next, since our problem is a prediction problem, a typical setup is to split the whole dataset into **training** and **testing** sets.</span>**.\n\nWith sciki-learn, we can do this step using [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) or [`StratifiedShuffleSplit`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from sklearn.model_selection import train_test_split", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "train_X, test_X, train_y, test_y = train_test_split(pca_X, y, test_size = 0.3, random_state=2)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print(\"Number of training samples:\\t\" + str(len(train_X)))\nprint(\"Number of testing samples:\\t\" + str(len(test_X)))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "---\n### <span style=\"color:#0b486b\">Step 2. Choose one or more suitable machine learing models for your task.</span>\n\nAt this step, we need to select a suitable or a set of suitable machine learning models for your problem at hand and train them using the **training dataset**. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<img src=\"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/note.gif\", width=\"40\", align=\"left\">This step requires deep understanding of machine learning models avaiable to you, when it works and it doesn't work, what input is expected, what are its pitfalls, how to intepret its outputs and so on. This [**scikit-learn cheat sheet**](http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html), for example, is a great place to start.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "In this tutorial, we will *logisticRegression* implemented in scikit-learn as the prediction model. Its description can be found [here](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "**<span style=\"color:blue\">2.1 Choose a sensible prediction model.</span>**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from sklearn.linear_model import LogisticRegression", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# create a logistic regression\nlogistic = LogisticRegression(penalty='l1', dual=False,\\\n              tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1,\\\n              class_weight=None, random_state=None, solver='liblinear', \\\n            max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "**<span style=\"color:blue\">2.2 Now we train the model using *training dataset*.</span>**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# train model\ntrain_model = logistic.fit(train_X, train_y)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print(train_model.coef_)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "---\n### <span style=\"color:#0b486b\">Step 3. Assessing the model performance using *testing dataset*.</span>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "**<span style=\"color:blue\">3.1 Use the trained model to predict newly unseen data samples in the testing datasets.</span>**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# perform prediction on the newly, unseen, test data.\npred_results = train_model.decision_function(test_X)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# examine what was returned\nprint(pred_results)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# LogisticRegression model implemeted in scikit-learn package employs a one-versus all strategy \n# for multilabel classification problem returns a array of log-likelihood prediction for each label\n# Here, we need to use the argmax function to return the most likely predictive label.\npred_labels = [np.argmax(pred_results[i]) for i in range(len(pred_results))]", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# print the first few predicted labels to gain a sense of how the model is performing\nprint(pred_labels[0:10])", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# print the first few prediction against the true labels\nfor i in range(10):\n    print(str(test_y[i]) + \" predicted as \" + str(pred_labels[i]))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "**<span style=\"color:blue\">3.2 Generate a report against model assessment and performances metrics.</span>**\nThe performance metrics can be different depending on the task at hand. Typically, it falls into either a **classification** or **regression** problems. \n\nOur current is face recognition, hence it is a classification problem. To assess the performance of a classification problem, important metrics include: **precision**, **recall**, and **F-score** which can be derived from the **confusion_matrix**.\n\nScikit-learn has provided several pre-implementated modules to compute these metrics, which can be founded [here](http://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# print out the confusion matrix\nprint(metrics.confusion_matrix(pred_labels, test_y))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# print out the classification report\ntarget_names=[\"person \" + str(i) for i in range(len(np.unique(y)))]\nprint(metrics.classification_report(pred_labels, test_y, target_names=target_names))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## <span style=\"color:#0b486b\">2. Summary </span>\nUp to now, you should understand the general principle as well concrete steps to build a typical machine learning model. Given a training dataset, these main steps can be summarized as follows:\n\n* **<span style=\"color:#0b486b\"> Step 1. Preparing your dataset: pre-processing, training and testing datasets</span>**\n    * 1.1 load the dataset\n    * 1.2 perform necessary feature pro-processing steps.\n    * 1.3 construct the training and testing sets. A typical spliting proportion is 70% for training and 30% for testing.\n* **<span style=\"color:#0b486b\">Step 2. Choose a suitable machine learning model for your problem at hand.</span>**\n    * 2.1 Pick a sensible model and initialize it\n    * 2.2 Train this model using **training** set\n* **<span style=\"color:#0b486b\">Step 3. Assess the performance of your modelling choice using **testing** dataset.</span>**\n    * 3.1 Use trained model to predict newly and unseen data samples from **testing** dataset.\n    * 3.2 Produce a performance report against model assessment metrics depending on which type of machine learning problems you are working with (e.g., classification or regression).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## <span style=\"color:#0b486b\">3. Put It All Together</span>\nFollow the summary above and put all above code segments together we end up the following piece of codes for our face detection problem.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<img src=\"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/note.gif\" width=\"40\", align=\"left\"></img>*Before proceeding to execute the following codes, you might want to start from everything from scatch. One way to do this is to **restart the kernel** by **pressing the digit '0' twice** from your keyboard.*", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# this function is a utility to face images from the dataset\ndef display_faces(images, label, num2display):\n    fig = plt.figure(figsize=(15,15))\n    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n    for i in range(num2display):\n        p = fig.add_subplot(20,20,i+1,xticks=[],yticks=[])\n        p.imshow(images[i], cmap=plt.cm.bone)\n        \n        p.text(0, 14, str(label[i]))\n        p.text(0, 60, str(i))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "from sklearn import datasets\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import decomposition\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# load face datasets\nfaces = datasets.fetch_olivetti_faces()\n\n# display the first 20 images\ndisplay_faces(faces.images, faces.target, 20)\n\n# store features in variable X and the label in variable y as in our usual convention notation.\nX, y = faces.data, faces.target\n\n# number of class labels\nn_classes = len(np.unique(y))\n\n# this steps further pre-process the features by performing a dimentionality reduction step via PCA.\npca = decomposition.PCA(n_components=10)\npca_X = pca.fit_transform(X)\n\n# split into training and testing sets\ntrain_X, test_X, train_y, test_y = train_test_split(pca_X, y, test_size = 0.3, random_state=2)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# create a logistic regression\nlogistic = LogisticRegression(penalty='l1', dual=False,\\\n              tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1,\\\n              class_weight=None, random_state=None, solver='liblinear', \\\n            max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)\n\n# train model\ntrain_model = logistic.fit(train_X, train_y)\n\n# perform prediction on the newly, unseen, test data.\npred_results = train_model.decision_function(test_X)\n\n# LogisticRegression model implemeted in scikit-learn package employs a one-versus all strategy \n# for multilabel classification problem returns a array of log-likelihood prediction for each label\n# Here, we need to use the argmax function to return the most likely predictive label.\npred_labels = [np.argmax(pred_results[i]) for i in range(len(pred_results))]\n\n# print out the classification report\ntarget_names=[\"person \" + str(i) for i in range(len(np.unique(y)))]\nprint(metrics.classification_report(pred_labels, test_y, target_names=target_names))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## <span style=\"color:#0b486b\">4. Automate Machine Learning Pipline</span>\nScikit-learn provides a Pipeline utility to help automate machine learning pipeline which executes a linear sequence of data transformation and model fitting to be chained together. The following code summarizes and packs above steps into a machine learning pipeline for our face detection problem.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from sklearn import datasets\nfrom sklearn import decomposition\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline\n\n\n# load face datasets\nfaces = datasets.fetch_olivetti_faces()\n\n# store features in variable X and the label in variable y as in our usual convention notation.\nX, y = faces.data, faces.target\n\n# create pipeline\nestimators = []\nestimators.append(('pca', decomposition.PCA(n_components=10)))\nestimators.append(('logistic', LogisticRegression(penalty='l1')))\nmodel = Pipeline(estimators)\n\n# evaluate pipeline\nkfold = StratifiedShuffleSplit(n_splits=5,test_size = 0.3) # split into training and testing sets with n_splits times.\n\nresults = cross_val_score(model, X, y, cv=kfold) # run the model with n_splits datasets and evaluate output.\n\nprint(results) # print accuracy for n_splits times of data\nprint(results.mean())", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## <span style=\"color:#0b486b\">5. Practical Coding Exercises</span>\n\n1. In section C, what is the performance if we don't perform the dimensionality reduction step? Will the precision of the face recognition problem increase or decrease? Use the above codes as the template, write your own codes to demonstrate this.\n2. Learning the lesson from step (1) above, vary the dimensionality in {2, 5, 10, 15, and 20} (e.g., the parameter `n_components`). Which number of components give the best results?\n3. Logistic regression is not the best predictive model choice for this problem. Read the description for the Support Vector Machines (SVM) from scikit-learn [here](http://scikit-learn.org/stable/modules/svm.html). What is the performance now if the classifier is SVM instead of LogisticRegression?. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<div align=\"right\">\n<a href=\"#matmul1\" class=\"btn btn-default\" data-toggle=\"collapse\">Click here for the solution to (3)</a>\n</div>\n<div id=\"matmul1\" class=\"collapse\">\n```\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import decomposition\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\n    \n# load face datasets\nfaces = datasets.fetch_olivetti_faces()\n\n# store features in variable X and the label in variable y as in our usual convention notation.\nX, y = faces.data, faces.target\n\n# number of class labels\nn_classes = len(np.unique(y))\n\n# this steps further pre-process the features by performing a dimentionality reduction step via PCA.\npca = decomposition.PCA(n_components=10)\npca_X = pca.fit_transform(X)\n\n# split into training and testing sets\ntrain_X, test_X, train_y, test_y = train_test_split(pca_X, y, test_size = 0.3, random_state=2)\n\n\n# create an SVM classifier\nsvm_model = svm.SVC(kernel='linear', C=1.0, cache_size=200, class_weight=None, coef0=0.0,\\\n                    decision_function_shape='ovo', degree=3, gamma='auto', \\\n                    max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False)\n\n# train model\ntrain_model = svm_model.fit(train_X, train_y)\n\n# perform prediction on the newly, unseen, test data.\npred_results = train_model.predict(test_X)\n\n# the predict function gives back the predicted label\npred_labels = pred_results\n\n\nprint(pred_labels)\n\n# print out the classification report\ntarget_names=[\"person \" + str(i) for i in range(len(np.unique(y)))]\nprint(metrics.classification_report(pred_labels, test_y, target_names=target_names))\n```\n</div>", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}