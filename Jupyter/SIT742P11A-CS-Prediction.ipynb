{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Modern Data Science \n**(Module 11: Data Analytics)**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are free to use, change and distribute this package.\n\nPrepared by and for \n**Student Members** |\n2006-2018 [TULIP Lab](http://www.tulip.org.au), Australia\n\n---\n\n\n## Session 11A - Case Study: Prediction\n\n\nThe purpose of this session is to demonstrate different coefficient and linear regression.\n\n\n### Content\n\n### Part 1 Linear Regression\n\n1.1 [Linear Regression Package](#lrp)\n\n1.2 [Evaluation](#eva)\n\n\n### Part 2 Classificiation\n\n2.1 [Skulls Dataset](#data)\n\n2.2 [Data Preprocessing](#datapre)\n\n2.3 [KNN](#knn)\n\n2.4 [Decision Tree](#dt)\n\n2.5 [Random Forest](#rf)\n\n\n---\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "\n## <span style=\"color:#0b486b\">1. Linear Regression</span>\n\n<a id = \"lrp\"></a>\n### <span style=\"color:#0b486b\">1.1 Linear Regression Package</span>\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "We will learn how to use sklearn package to do linear regression", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from sklearn.datasets import load_diabetes \nfrom sklearn.linear_model import LinearRegression \nimport matplotlib.pyplot as plt \n%matplotlib inline", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now create an instance of the diabetes data set by using the <b>load_diabetes</b> function as a variable called <b>diabetes</b>.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "diabetes = load_diabetes()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "We will work with one feature only.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "diabetes_X = diabetes.data[:, None, 2]", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now create an instance of the LinearRegression called LinReg.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "LinReg = LinearRegression()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now to perform <b>train/test split</b> we have to split the <b>X</b> and <b>y</b> into two different sets: The <b>training</b> and <b>testing</b> set. Luckily there is a sklearn function for just that!\n\nImport the <b>train_test_split</b> from <b>sklearn.cross_validation</b>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from sklearn.cross_validation import train_test_split", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now <b>train_test_split</b> will return <b>4</b> different parameters. We will name this <b>X_trainset</b>, <b>X_testset</b>, <b>y_trainset</b>, <b>y_testset</b>. \n\nNow let's use <b>diabetes_X</b> as the <b>Feature Matrix</b> and <b>diabetes.target</b> as the <b>response vector</b> and split it up using <b>train_test_split</b> function we imported earlier (<i>If you haven't, please import it</i>). The <b>train_test_split</b> function should have <b>test_size = 0.3</b> and a <b>random state = 7</b>. \n\n\nThe <b>train_test_split</b> will need the parameters <b>X</b>, <b>y</b>, <b>test_size=0.3</b>, and <b>random_state=7</b>. The <b>X</b> and <b>y</b> are the arrays required before the split, the <b>test_size</b> represents the ratio of the testing dataset, and the <b>random_state</b> ensures we obtain the same splits.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "X_trainset, X_testset, y_trainset, y_testset = train_test_split(diabetes_X, diabetes.target, test_size=0.3, random_state=7)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Train the <b>LinReg</b> model using <b>X_trainset</b> and <b>y_trainset</b>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "LinReg.fit(X_trainset, y_trainset)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now let's <i>plot</i> the graph.\n<p> Use plt's <b>scatter</b> function to plot all the datapoints of <b>X_testset</b> and <b>y_testset</b> and color it <b>black</b> </p>\n<p> Use plt's <b>plot</b> function to plot the line of best fit with <b>X_testset</b> and <b>LinReg.predict(X_testset)</b>. Color it <b>blue</b> with a <b>linewidth</b> of <b>3</b>. </p> <br>\n<b>Note</b>: Please ignore the FutureWarning. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "plt.scatter(X_testset, y_testset, color='black')\nplt.plot(X_testset, LinReg.predict(X_testset), color='blue', linewidth=3)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "\n<a id = \"eva\"></a>\n\n\n### <span style=\"color:#0b486b\">1.2 Evaluation</span>\n\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "In this part, you will learn the about the different evaluation models and metrics. You will be able to identify the strengths and weaknesses of each model and how to incorporate underfitting or overfilling them also referred to as the Bias-Variance trade-off.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import numpy as np", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "<br><b> Here's a list of useful functions: </b><br>\n    mean -> np.mean()<br>\n    exponent -> **<br>\n    absolute value -> abs()\n    \n    \nWe use three evaluation metrics:\n\n$$ MAE = \\frac{\\sum_{j=1}^n|y_i-\\hat y_i|}{n} $$\n\n$$ MSE = \\frac{\\sum_{j=1}^n (y_i-\\hat y_i)^2}{n} $$\n\n$$ RMSE = \\sqrt{\\frac{\\sum_{j=1}^n (y_i-\\hat y_i)^2}{n}} $$\n\n\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "print(np.mean(abs(LinReg.predict(X_testset) - y_testset)))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print(np.mean((LinReg.predict(X_testset) - y_testset) ** 2) )", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print(np.mean((LinReg.predict(X_testset) - y_testset) ** 2) ** (0.5) )", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "---\n## <span style=\"color:#0b486b\">2. Classification</span>\n\n<a id = \"cls\"></a>\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id = \"data\"></a>\n\n### <span style=\"color:#0b486b\">2.1 Skulls dataset</span>\n\nIn this section, we will take a closer look at a data set.\n\nEverything starts off with how the data is stored. We will be working with .csv files, or comma separated value files. As the name implies, each attribute (or column) in the data is separated by commas.\n\nNext, a little information about the dataset. We are using a dataset called skulls.csv, which contains the measurements made on Egyptian skulls from five epochs.\n\n#### The attributes of the data are as follows: \n\n<img src = \"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/Skull.png\", align = 'left'>\n\n<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>\n\n<b>epoch</b> - The epoch the skull as assigned to, a factor with levels c4000BC c3300BC, c1850BC, c200BC, and cAD150, where the years are only given approximately.\n\n<b>mb</b> - Maximal Breadth of the skull.\n\n<b>bh</b> - Basiregmatic Heights of the skull.\n\n<b>bl</b> - Basilveolar Length of the skull.\n\n<b>nh</b> - Nasal Heights of the skull.\n\n#### Importing Libraries\n\nBefore we begin, we need to import some libraries, as they have useful functions that will be used later on.<br>\n\nIf you look at the imports below, you will notice the return of **numpy**! Remember that numpy is homogeneous multidimensional array (ndarray).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import numpy as np\nimport pandas", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "---\nWe need the **pandas** library for a function to read .csv files\n<ul>\n    <li> <b>pandas.read_csv</b> - Reads data into DataFrame </li>\n    <li> The read_csv function takes in <i>2 parameters</i>: </li>\n    <ul>\n        <li> The .csv file as the first parameter </li>\n        <li> The delimiter as the second parameter </li>\n    </ul>\n</ul>\n\n-----------------------------\nSave the \"<b> skulls.csv </b>\" data file into a variable called <b> my_data </b>  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import wget\n\nlink_to_data = 'https://github.com/tuliplab/mds/raw/master/Jupyter/data/skulls.csv'\nDataSet = wget.download(link_to_data)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "my_data = pandas.read_csv(\"skulls.csv\", delimiter=\",\")\nmy_data.describe()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Print out the data in <b> my_data </b>  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "print(my_data)", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Check the type of <b> my_data </b>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "print (type(my_data))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "-----------\nThere are various functions that the **pandas** library has to look at the data\n<ul>\n    <li> <font color = \"red\"> [DataFrame Data].columns </font> - Displays the Header of the Data </li>\n    <ul> \n        <li> Type: pandas.indexes.base.Index </li>\n    </ul>\n</ul>\n\n<ul>\n    <li> <font color = \"red\"> [DataFrame Data].values </font> (or <font color = \"red\"> [DataFrame Data].as_matrix() </font>) - Displays the values of the data (without headers) </li>\n    <ul>\n        <li> Type: numpy.ndarray </li>\n    </ul>\n</ul>\n\n<ul>\n    <li> <font color = \"red\"> [DataFrame Data].shape </font> - Displays the dimensions of the data (rows x columns) </li>\n    <ul>\n        <li> Type: tuple </li>\n    </ul>\n</ul>\n\n----------\nUsing the <b> my_data </b> variable containing the DataFrame data, retrieve the <b> header </b> data, data <b> values </b>, and <b> shape </b> of the data. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "print( my_data.columns)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print (my_data.values)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print (my_data.shape)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "<a id = \"datapre\"></a>\n\n### <span style=\"color:#0b486b\">2.2 Data Preprocessing</span>\n\nWhen we train a model, the model requires two inputs, X and y\n<ul>\n    <li> X: Feature Matrix, or array that contains the data. </li>\n    <li> y: Response Vector, or 1-D array that contains the classification categories </li>\n</ul>\n\n\n\n------------\nThere are some problems with the data in my_data:\n<ul>\n    <li> There is a header on the data (Unnamed: 0    epoch   mb   bh   bl  nh) </li>\n    <li> The data needs to be in numpy.ndarray format in order to use it in the machine learning model </li>\n    <li> There is non-numeric data within the dataset </li>\n    <li> There are row numbers associated with each row that affect the model </li>\n</ul>\n\nTo resolve these problems, I have created a function that fixes these for us:\n<b> removeColumns(pandasArray, column) </b>\n\nThis function produces one output and requires two inputs.\n<ul>\n    <li> 1st Input: A pandas array. The pandas array we have been using is my_data </li>\n    <li> 2nd Input: Any number of integer values (order doesn't matter) that represent the columns that we want to remove. (Look at the data again and find which column contains the non-numeric values). We also want to remove the first column because that only contains the row number, which is irrelevant to our analysis.</li>\n    <ul>\n        <li> Note: Remember that Python is zero-indexed, therefore the first column would be 0. </li>\n    </ul>\n</ul>\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Remove the column containing the target name since it doesn't contain numeric values.\n# Also remove the column that contains the row number\n# axis=1 means we are removing columns instead of rows.\n# Function takes in a pandas array and column numbers and returns a numpy array without\n# the stated columns\ndef removeColumns(pandasArray, *column):\n    return pandasArray.drop(pandasArray.columns[[column]], axis=1).values", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "---------\nUsing the function, store the values from the DataFrame data into a variable called new_data. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "new_data = removeColumns(my_data, 0, 1)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Print out the data in <b> new_data </b>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "print(new_data)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "-------\nNow, we have one half of the required data to fit a model, which is X or new_data\n\nNext, we need to get the response vector y. Since we cannot use .target and .target_names, I have created a function that will do this for us.\n\n<b> targetAndtargetNames(numpyArray, targetColumnIndex) </b>\n\nThis function produces two outputs, and requires two inputs.\n<ul>\n    <li> <font size = 3.5><b><i>1st Input</i></b></font>: A numpy array. The numpy array you will use is my_data.values (or my_data.as_matrix())</li>\n    <ul>\n        <li> Note: DO NOT USE <b> new_data </b> here. We need the original .csv data file without the headers </li>\n    </ul>\n</ul>\n<ul>\n    <li> <font size = 3.5><b><i>2nd Input</i></b></font>: An integer value that represents the target column . (Look at the data again and find which column contains the non-numeric values. This is the target column)</li>\n    <ul>\n        <li> Note: Remember that Python is zero-indexed, therefore the first column would be 0. </li>\n   </ul>\n</ul>\n\n<ul>\n    <li> <font size = 3.5><b><i>1st Output</i></b></font>: The response vector (target) </li>\n    <li> <font size = 3.5><b><i>2nd Output</i></b></font>: The target names (target_names) </li>\n</ul>\n\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "def targetAndtargetNames(numpyArray, targetColumnIndex):\n    target_dict = dict()\n    target = list()\n    target_names = list()\n    count = -1\n    for i in range(len(my_data.values)):\n        if my_data.values[i][targetColumnIndex] not in target_dict:\n            count += 1\n            target_dict[my_data.values[i][targetColumnIndex]] = count\n        target.append(target_dict[my_data.values[i][targetColumnIndex]])\n    # Since a dictionary is not ordered, we need to order it and output it to a list so the\n    # target names will match the target.\n    for targetName in sorted(target_dict, key=target_dict.get):\n        target_names.append(targetName)\n    return np.asarray(target), target_names", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Using the targetAndtargetNames function, create two variables called <b>target</b> and <b>target_names</b>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "y, targetNames = targetAndtargetNames(my_data, 1)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Print out the <b>y</b> and <b>targetNames</b> variables you created.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "print(y)\nprint(targetNames)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now that we have the two required variables to fit the data, a sneak peak at how to fit data will be shown in the cell below.\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id = \"knn\"></a>\n\n### <span style=\"color:#0b486b\">2.3 KNN</span>\n\n**K-Nearest Neighbors** is an algorithm for supervised learning. Where the data is 'trained' with data points corresponding to their classification. Once a point is to be predicted, it takes into account the 'K' nearest points to it to determine it's classification.\n\n#### Here's an visualization of the K-Nearest Neighbors algorithm.\n\n<img src = \"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/KNN.png\">\n\nIn this case, we have data points of Class A and B. We want to predict what the star (test data point) is. If we consider a k value of 3 (3 nearest data points) we will obtain a prediction of Class B. Yet if we consider a k value of 6, we will obtain a prediction of Class A.\n\nIn this sense, it is important to consider the value of k. But hopefully from this diagram, you should get a sense of what the K-Nearest Neighbors algorithm is. It considers the 'K' Nearest Neighbors (points) when it predicts the classification of the test point.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# X = removeColumns(my_data, 0, 1)\n# y = target(my_data, 1)\n\nX = new_data\n\nprint( X.shape)\nprint (y.shape)\n\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now to perform <b>train/test split</b> we have to split the <b>X</b> and <b>y</b> into two different sets: The <b>training</b> and <b>testing</b> set. Luckily there is a sklearn function for just that!\n\nImport the <b>train_test_split</b> from <b>sklearn.cross_validation</b>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from sklearn.cross_validation import train_test_split", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now <b>train_test_split</b> will return <b>4</b> different parameters. We will name this <b>X_trainset</b>, <b>X_testset</b>, <b>y_trainset</b>, <b>y_testset</b>. The <b>train_test_split</b> will need the parameters <b>X</b>, <b>y</b>, <b>test_size=0.3</b>, and <b>random_state=7</b>. The <b>X</b> and <b>y</b> are the arrays required before the split, the <b>test_size</b> represents the ratio of the testing dataset, and the <b>random_state</b> ensures we obtain the same splits.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "X_trainset, X_testset, y_trainset, y_testset = train_test_split(X, y, test_size=0.3, random_state=7)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now let's print the shape of the training sets to see if they match.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "print (X_trainset.shape)\nprint (y_trainset.shape)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Let's check the same with the testing sets! They should both match up!", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "print (X_testset.shape)\nprint (y_testset.shape)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now similarly with the last lab, let's create declarations of KNeighborsClassifier. Except we will create 3 different ones:\n\n- neigh -> n_neighbors = 1 \n- neigh23 -> n_neighbors = 23 \n- neigh90 -> n_neighbors = 90 ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from sklearn.neighbors import KNeighborsClassifier\n\nneigh = KNeighborsClassifier(n_neighbors = 1)\nneigh23 = KNeighborsClassifier(n_neighbors = 23)\nneigh90 = KNeighborsClassifier(n_neighbors = 90)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now we will fit each instance of <b>KNeighborsClassifier</b> with the <b>X_trainset</b> and <b>y_trainset</b>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "neigh.fit(X_trainset, y_trainset)\nneigh23.fit(X_trainset, y_trainset)\nneigh90.fit(X_trainset, y_trainset)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now you are able to predict with <b>multiple</b> datapoints. We can do this by just passing in the <b>y_testset</b> which contains multiple test points into a <b>predict</b> function of <b>KNeighborsClassifier</b>.\n\nLet's pass the <b>y_testset</b> in the <b>predict</b> function each instance of <b>KNeighborsClassifier</b> but store it's returned value into <b>pred</b>, <b>pred23</b>, <b>pred90</b> (corresponding to each of their names)\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "pred = neigh.predict(X_testset)\npred23 = neigh23.predict(X_testset)\npred90 = neigh90.predict(X_testset)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Awesome! Now let's compute neigh's <b>prediction accuracy</b>. We can do this by using the <b>metrics.accuracy_score</b> function", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from sklearn import metrics\nprint(\"Neigh's Accuracy: \"), metrics.accuracy_score(y_testset, pred)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Interesting! Let's do the same for the other instances of KNeighborsClassifier.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "print(\"Neigh23's Accuracy: \"), metrics.accuracy_score(y_testset, pred23)\nprint(\"Neigh90's Accuracy: \"), metrics.accuracy_score(y_testset, pred90)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "As shown, the accuracy of <b>neigh23</b> is the highest. When <b>n_neighbors = 1</b>, the model was <b>overfit</b> to the training data (<i>too specific</i>) and when <b>n_neighbors = 90</b>, the model was <b>underfit</b> (<i>too generalized</i>). In comparison, <b>n_neighbors = 23</b> had a <b>good balance</b> between <b>Bias</b> and <b>Variance</b>, creating a generalized model that neither <b>underfit</b> the data nor <b>overfit</b> it.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id = \"dt\"></a>\n\n### <span style=\"color:#0b486b\">2.4 Decision Tree</span>\n\nIn this section, you will learn <b>decision trees</b> and <b>random forests</b>. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "The <b> getFeatureNames </b> is a function made to get the attribute names for specific columns\n\nThis function produces one output and requires two inputs:\n<ul>\n    <li> <b>1st Input</b>: A pandas array. The pandas array we have been using is <b>my_data</b>. </li>\n    <li> <b>2nd Input</b>: Any number of integer values (order doesn't matter) that represent the columns that we want to include. In our case we want <b>columns 2-5</b>. </li>\n    <ul> <li> Note: Remember that Python is zero-indexed, therefore the first column would be 0. </li> </ul>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "def getFeatureNames(pandasArray, *column):\n    actualColumns = list()\n    allColumns = list(pandasArray.columns.values)\n    for i in sorted(column):\n        actualColumns.append(allColumns[i])\n    return actualColumns", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now we prepare the data for decision tree construction.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#X = removeColumns(my_data, 0, 1) \n#y, targetNames = targetAndtargetNames(my_data, 1) \nfeatureNames = getFeatureNames(my_data, 2,3,4,5)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Print out <b>y</b>, <b>targetNames</b>, and <b>featureNames</b> to use in the next example. Remember that the numbers correspond to the names, 0 being the first name,1 being the second name, and so on.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "print( y )\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print (targetNames )\n\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print (featureNames)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "We will first create an instance of the <b>DecisionTreeClassifier</b> called <b>skullsTree</b>.<br>\nInside of the classifier, specify <i> criterion=\"entropy\" </i> so we can see the information gain of each node.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from sklearn.tree import DecisionTreeClassifier", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "skullsTree = DecisionTreeClassifier(criterion=\"entropy\")", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "skullsTree.fit(X_trainset,y_trainset)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Let's make some <b>predictions</b> on the testing dataset and store it into a variable called <b>predTree</b>.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "predTree = skullsTree.predict(X_testset)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "You can print out <b>predTree</b> and <b>y_testset</b> if you want to visually compare the prediction to the actual values.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "print (predTree) \nprint (y_testset)", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Next, let's import metrics from sklearn and check the accuracy of our model.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from sklearn import metrics\nprint(\"DecisionTrees's Accuracy: \"), metrics.accuracy_score(y_testset, predTree)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now we can visualize the tree constructed.\n\nHowever, it should be noted that the following code may not work in all Python2 environment. You can try to see packages like <b>pydot</b>, <b>pydot2</b>, <b>pydot2</b>, <b>pydotplus</b>, etc., and see which one works in your platform.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "!pip install pydotplus\n#!pip install pydot2\n\n#!pip install pyparsing==2.2.0\n!pip install pydot", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "!conda install sklearn", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "from IPython.display import Image  \nfrom sklearn.externals.six import StringIO\nfrom sklearn import tree\nimport pydot\nimport pydotplus\nimport pandas as pd\n\n\ndot_data = StringIO()  \n\ntree.export_graphviz(skullsTree, out_file=dot_data, \nfeature_names=featureNames, \nclass_names=targetNames, \nfilled=True, rounded=True, \nspecial_characters=True, \nleaves_parallel=True)\n\n\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\nImage(graph.create_png())  ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "<a id = \"rf\"></a>\n\n### <span style=\"color:#0b486b\">2.5 Random Forest</span>\n\nImport the <b>RandomForestClassifier</b> class from <b>sklearn.ensemble</b>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from sklearn.ensemble import RandomForestClassifier", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Create an instance of the <b>RandomForestClassifier()</b> called <b>skullsForest</b>, where the forest has <b>10 decision tree estimators</b> (<i>n_estimators=10</i>) and the <b>criterion is entropy</b> (<i>criterion=\"entropy\"</i>)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "skullsForest=RandomForestClassifier(n_estimators=10, criterion=\"entropy\")", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Let's use the same <b>X_trainset</b>, <b>y_trainset</b> datasets that we made when dealing with the <b>Decision Trees</b> above to fit <b>skullsForest</b>.\n<br> <br>\n<b>Note</b>: Make sure you have ran through the Decision Trees section.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "skullsForest.fit(X_trainset, y_trainset)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Let's now create a variable called <b>predForest</b> using a predict on <b>X_testset</b> with <b>skullsForest</b>.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "predForest = skullsForest.predict(X_testset)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "You can print out <b>predForest</b> and <b>y_testset</b> if you want to visually compare the prediction to the actual values.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "print (predForest )\nprint (y_testset)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Let's check the accuracy of our model. <br>\n\nNote: Make sure you have metrics imported from sklearn", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "print(\"RandomForests's Accuracy: \"), metrics.accuracy_score(y_testset, predForest)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "We can also see what trees are in our <b> skullsForest </b> variable by using the <b> .estimators_ </b> attribute. This attribute is indexable, so we can look at any individual tree we want.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "print(skullsForest.estimators_)", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "You can choose to view any tree by using the code below. Replace the <i>\"&\"</i> in <b>skullsForest[&]</b> with the tree you want to see.\n\nThe following block may not work in your Python enrionment.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from IPython.display import Image  \nfrom sklearn.externals.six import StringIO  \nimport pydot\ndot_data = StringIO()\n\n#Replace the '&' below with the tree number\ntree.export_graphviz(skullsForest[&], out_file=dot_data,\n                     feature_names=featureNames,\n                     class_names=targetNames,\n                     filled=True, rounded=True,\n                     special_characters=True,\n                     leaves_parallel=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\nImage(graph.create_png())  ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }, 
        "anaconda-cloud": {}
    }, 
    "nbformat": 4
}