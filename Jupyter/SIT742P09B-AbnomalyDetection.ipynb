{
    "nbformat_minor": 2, 
    "cells": [
        {
            "source": "# SIT742: Modern Data Science \n**(Week 09: Data Analytics (II))**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are free to use, change and distribute this package.\n- If you found any issue/bug for this document, please submit an issue at [tulip-lab/sit742](https://github.com/tulip-lab/sit742/issues)\n\nPrepared by **SIT742 Teaching Team**\n\n---\n\n## Session 9B - Abnormality Analytics\n\n**The purpose of this session is to demonstrate:**\n\n1. To understand characteristics of data in anomaly and novel detection problems\n2. How to implement standard anomaly and novel detection algorithms \n\n** References and additional reading and resources**\n- [Novelty and outlier detection with scikit-learn](http://scikit-learn.org/stable/modules/outlier_detection.html)\n\n---\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": " # <span style=\"color:#0b486b\">1. Challenges with anomaly and novel detection datasets</span> ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "When dealing with anomaly and novel detection problems, you usually encounter the following challenges:\n1. High dimensional data which contains a relatively large number of attributes. It is not possible to plot data points for getting the sense of data.\n2. Imbalanced data whose class distribution is not (approximately) equal between two classes: normal and abnormal. For example, in the application of credit card fraud detection, the number of fraud transactions usually covers only 5% to 10% of entire datasets. Therefore, the standard classification algorithms are usually failed to apply into anomaly and novel detection problems.\n\nIn this section, you would have a chance to explore the following datasets\n- [EMNIST Dataset](https://www.nist.gov/itl/iad/image-group/emnist-dataset) is a set of handwritten digits and characters, each of which is a 28x28 pixel image. In this practical application, we subsample about 10% of characters which are considered as abnormal data points. The digits are treated as normal data. You can think of a real-world application of this dataset as a handwritten form recognition for detecting phone numbers. \n\nYou can learn the different characteristics of these datasets in the following codes.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# <span style=\"color:#0b486b\"> 2. Anomaly Detection Dataset: EMNIST </span> ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": " *__EMNIST__* dataset contains many of digit images and a few of non-digit images. Our aim is to train a model using this dataset to detect non-digit images. We can apply this model to build a machine to verify a valid phone number written by hand. You now can load EMNIST data (in csv format) and view data properties using the following code. The first column represents labels of data instances. The rest are feature vectors of data instances.\n\nNow you can load data and get some basic information of dataset using <code>info()</code> function.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "!ls", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "import wget\n    \nlink_to_data = 'https://github.com/tulip-lab/sit742/blob/master/Jupyter/data/emnist.digits_letters.small.csv?raw=true'\nDataSet = wget.download(link_to_data) ", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "import numpy as np\nimport pandas as pd\ndf = pd.read_csv('emnist.digits_letters.small.csv',index_col=0)", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df.info()\n", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df = df.sort_values(['0'])  # for further visualization\n\n", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "df = df.sort_values(['0'])  # for further visualization\ndf.info()", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true, 
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "You can also have a look at the top five rows using the DataFrame\u2019s <code>head()</code> method.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df.head()", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "For using *numpy* array methods, you can convert the data frame *df* to a numpy array object *data_array*.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "data_array = df.as_matrix()\nx = data_array[:,1:]\ny = data_array[:,0]\n\nnum_samples = x.shape[0]\n\nprint(\"Feature matrix for the first 5 images\\n {}\".format(x[:5,:]))\nprint(\"\\nLabels for the first 5 images\\n {}\".format(y[:5]))\n\nprint(\"\\nNumber of samples: {}\".format(num_samples))\n", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Since the dataset contains images, you can sample and plot some digits images (labeled as 1) in the dataset. Note that you need to reshape to a matrix before using imshow to view this image because the feature vector is flattened in 1D.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "%pylab inline\npylab.rcParams['figure.figsize'] = (10, 6) # configure the size of images\n\nimport matplotlib.pyplot as plt \nnum_subplots = 5                                      # the number of images plotted\nfig, ax = plt.subplots(1,num_subplots)\nfor idx in range(num_subplots):\n    n = np.random.randint(np.sum(y < 0), len(y))      # randomly choose an image index\n    img1 = x[n,:].reshape((28,28)).T                  # reshape the vector into the image size 28x28\n    ax[idx].imshow(img1, cmap = plt.get_cmap('gray')) # show the selected image\nplt.show()", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Similarly, you can sample and plot some non-digit images in the dataset.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "%pylab inline\npylab.rcParams['figure.figsize'] = (10, 6)\n\nimport matplotlib.pyplot as plt\nnum_subplots = 5\nfig, ax = plt.subplots(1,num_subplots)                 # the number of images plotted\nfor idx in range(5):\n    n = np.random.randint(0, np.sum(y < 0))            # randomly choose an image index\n    img1 = x[n,:].reshape((28,28)).T                   # reshape the vector into the image size 28x28\n    ax[idx].imshow(img1, cmap = plt.get_cmap('gray'))  # show the selected image\nplt.show()", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## Dataset Statistics", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "First, we examine the ratio between normal data (labeled as **'1'**) and abnormal data (labeled as **'-1'**). Intuitively, we can see it is an imbalanced dataset.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import matplotlib.pyplot as plt\n(counts, _) = np.histogram(y,2)\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.bar([0,1], counts)\nclasslabels=[\"abnormal\",\"normal\"]\n\nrects =ax.patches\n\n# Now make labels of percentage\nlabels = ['{:.3f}%'.format(i*100) for i in counts/np.sum(counts)]\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n#     plt.text(1, 173 , \"dadad\")\n    ax.text(rect.get_x() + rect.get_width()/2, height + 2, label, ha='center', va='bottom')\n\nplt.ylabel(\"Count\")\nplt.xticks(np.arange(2),classlabels)\nplt.show()", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "You also visualize data in 2D using PCA approach. In particular, You project it in the first two principal components space and plot it using scatter function.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from sklearn.decomposition import PCA\n\nx_reduced = PCA(n_components=2).fit_transform(x) # reduce data dimension to 2\n\nplt.figure(2, figsize=(8, 6))\nplt.scatter(x_reduced[:, 0], x_reduced[:, 1], c=y, cmap='PiYG')  # plot 2-d data where each data point is decorated with its label.\nplt.show()", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# <span style=\"color:#0b486b\"> 3.  Anomaly Detection Systems using Classifier</span> ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "The dataset is provided with labels, each of which is categorical data. Therefore, we can apply classification algorithms to learn and predict. We can use *Logistic Regression* and *Naive Bayes* models for our systems. We can report the performance of chosen method in terms of *accuracy, precision, recall, and F-measure*.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from sklearn import decomposition\npca = decomposition.PCA(n_components=100)\npca_X = pca.fit_transform(x)\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\n\nX_train, X_test, y_train, y_test = train_test_split(pca_X, y, test_size = 0.3, random_state=2)\nlogistic = LogisticRegression()\nlogistic.fit(X_train, y_train)\n\ny_prediction=logistic.predict(X_test )\n\nrec=recall_score(y_test,y_prediction, average='macro')\npre=precision_score(y_test,y_prediction, average='macro')\nacc=accuracy_score(y_test,y_prediction)\nf1=f1_score(y_test,y_prediction, average='macro')\nprint(\"\\t\\t\\tAccuracy\\tPrecision\\tRecall\\t\\tF-measure\")\nprint(\"Logistic Regression\\t{:f}\\t{:f}\\t{:f}\\t{:f}\".format(acc,pre,rec,f1))\n\n\n\nfrom sklearn.naive_bayes import GaussianNB\n\nmodel=GaussianNB()\nmodel.fit( X_train, y_train )\ny_prediction = model.predict( X_test )\n\nrec=recall_score(y_test,y_prediction, average='macro')\npre=precision_score(y_test,y_prediction, average='macro')\nacc=accuracy_score(y_test,y_prediction)\nf1=f1_score(y_test,y_prediction, average='macro')\nprint(\"Naive Bayes\\t\\t{:f}\\t{:f}\\t{:f}\\t{:f}\".format(acc,pre,rec,f1))\n\n", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false, 
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "We can observe that the accuracy is always high in this problem since the nature of data. If we choose all data point as normal, the accuracy now is the proportion of normal class, ~ 91%., similar to Naive Bayes classifier. However, the other metrics are not good. We need a class of better algorithms to deal with imbalanced data in anomaly detection problems.\nIn week 3 and 4, we present three algorithms designed to cope with this problem:\n- Two distance-based approaches: $DB(p,d)$ and $DB(k,N)$\n- PCA-based approach.\n\nIn the following section, we introduce you two distance-based approaches: $DB(p,d)$ and $DB(k,N)$.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# <span style=\"color:#0b486b\"> 4.  Anomaly Detection Systems with Specialized Design Algorithms </span> ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## 4.1 Distance-based approaches", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 4.1.1 $DB(p,d)$ algorithm ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "In  $DB(p,d)$  algorithm, an object  $o$  is an anomaly if at least a fraction  $p$  of objects in dataset has distances greater than  $d$  from  oo.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import numpy as np\nimport pandas as pd\nfrom sklearn.metrics.pairwise import pairwise_distances\nfrom sklearn import metrics\n", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Then, we calculate distance matrix.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# calculate distance matrix\ndist_matrix = pairwise_distances(x, metric='euclidean')\nnp.max(dist_matrix)", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now, we need to set model hyper-parameters   $d$  and  $p$ and compute the proportion of objects in dataset have distances greater than $d$ from a given data point. If it is greater than $p$, we mark it as an anomaly.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# set model hyper-parameters\nd = 74.0\np = 0.009\n\ndist_matrix_greater_d = dist_matrix > d\nsum_dist_matrix_greater_d = np.sum(dist_matrix_greater_d, axis=1)\npercent_greater_d = sum_dist_matrix_greater_d / (num_samples - 1)\n\ny_predict = np.ones(num_samples)\ny_predict[percent_greater_d > p] = -1", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "It is used as anomaly score. The higher anomaly score is, the most likely it is an anomaly.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import matplotlib.pyplot as plt\n\nplt.figure(2, figsize=(12, 8))\n\ndata_idx = np.arange(num_samples)\nidx_anomaly = data_idx[percent_greater_d > p]\n\nplt.scatter(data_idx, sum_dist_matrix_greater_d,s=3)\nplt.scatter(data_idx[percent_greater_d > p], sum_dist_matrix_greater_d[percent_greater_d > p],s=3)\nthreshold_line = np.ones(num_samples) * np.min(sum_dist_matrix_greater_d[idx_anomaly])\nplt.plot(data_idx, threshold_line, color='green', linewidth=1.5)\nplt.yscale('log')\nplt.show()", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print('Classification results:')\nprint(metrics.classification_report(y, y_predict))\n\nconfusion_mat = metrics.confusion_matrix(y, y_predict, [1, -1])\nprint('Confusion matrix')\ndf_confusion = pd.DataFrame(confusion_mat, columns=['Prediction Positive ','Prediction Negative'])\ndf_confusion.index = ['Original Positive','Original Negative']\ndf_confusion", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "### 4.1.2 $DB(k,N)$ algorithm ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "\nNow, we implement $DB(k,N)$ algorithm. \n - **Input**: $k$ (the number of nearest neighbours), $N$ (the number of anomalies) and the dataset\n - **Output**: anomalies in the dataset\n \nFirst, we load dataset.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import numpy as np\nimport pandas as pd\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn import metrics\n\n# load data\ndf = pd.read_csv('emnist.digits_letters.small.csv',index_col=0)\ndata_array = df.as_matrix()\nx = data_array[:,1:]\n\nnum_samples = x.shape[0]\nprint('Number of samples:', num_samples)", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "In  $DB(k,N)$ algorithm, an object  oo  is an anomaly if it is in top  $N$  data instances whose distances to its  $k$  nearest neighbours are largest. Now, we need to set model hyper-parameters  $k$  and  $N$. Then for each data instance, we find $k$ nearest neighbours.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# set model hyper-parameters\nk = 10\nN = 50\n# find k-NN\nnbrs = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(x)\ndistances, indices = nbrs.kneighbors(x)\navg_distances = np.average(distances, axis=1)", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "It is used as anomaly score. The higher anomaly score is, the most likely it is an anomaly. We mark top $N$ data instance that have the largest distance to $k$ nearest neighbours.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# get top N far from neighbours\nplt.figure(2, figsize=(12, 8))\n\n\nidx_anomaly = avg_distances.argsort()[-N:][::-1]\nplt.scatter(np.arange(num_samples),avg_distances,s=3)\nplt.scatter(idx_anomaly,avg_distances[idx_anomaly],color='red',s=3)\nthreshold_line = np.ones(num_samples) * np.min(avg_distances[idx_anomaly])\nplt.plot(np.arange(num_samples), threshold_line, color='green',linewidth=1)\nplt.show()", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Finally, we report the prediction performance.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "y_predict = np.ones(num_samples)\ny_predict[idx_anomaly] = -1\nprint('Classification results:')\nprint(metrics.classification_report(y, y_predict))\n\nconfusion_mat = metrics.confusion_matrix(y, y_predict, [1, -1])\nprint('Confusion matrix')\ndf_confusion = pd.DataFrame(confusion_mat, columns=['Prediction Positive ','Prediction Negative'])\ndf_confusion.index = ['Original Positive','Original Negative']\ndf_confusion", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true, 
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "We also have a ROC plot.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from sklearn import metrics\nFPR, TPR, _ = metrics.roc_curve(y, avg_distances, pos_label=[1])\nplt.plot(TPR,FPR)\nplt.show()", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false, 
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## <span style=\"color:#0b486b\">V. Practical Coding Exercises</span>\n\n1. In section 3, can you report the performance for some other classification algorithms such as [K-NN](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) and [Decision Tree Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n2. You can try to vary $d$ and $p$ values in $DP(d,p)$ algorithm and $k$ and $N$ values for $DP(k,N)$ algorithm and report the best values for each algorithm in terms of F-measure.\n3. We provide you a subset of [Statlog (German Credit Data) Data Set](https://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)) in **german.csv**. You can try to understand the data statistics and use distance-based anomaly detection in Section 2 to 4 and report the results.", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }, 
        "anaconda-cloud": {}
    }, 
    "nbformat": 4
}